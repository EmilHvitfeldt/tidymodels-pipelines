{
  "hash": "90f03b9a0eff160f867c99e595cbf4a2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: AWS, CSV Input, SageMaker Endpoint\ndescription: \"Using vetiver to version a model on Amazon SageMaker as an endpoint, and predict from it\"\nimage: ../../images/csv-sagemaker-deploy.jpg\ncategories:\n  - csv\n  - vetiver\n  - Amazon SageMaker\n  - AWS\n---\n\n\n::: {.callout-note}\nThis page was last generated on 2024-02-29. If you find the code out of date please [file an issue](https://github.com/EmilHvitfeldt/tidymodels-pipelines/issues/new).\n:::\n\n::: new\n::: {.callout-note}\n## Changes from standard\n\nAll changes from the [standard pipeline](../standard/index.qmd) are highlighted with a cranberry line to the right.\n:::\n:::\n\n\n\n::: new\n\n## Loading packages\n\nWe are using the tidymodels package to do the modeling, [embed](https://embed.tidymodels.org/) for target encoding, [pins](https://pins.rstudio.com/) for versioning,  [vetiver](https://vetiver.rstudio.com/) for version and deployment, and [smdocker](https://dyfanjones.r-universe.dev/smdocker) for deploying to SageMaker.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"pak\")\n# pak::pak(\"tidymodels\", \"embed\", \"vetiver\", \"pins\", \"smdocker\")\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(smdocker)\n```\n:::\n\n\n:::\n\n## Loading Data\n\nWe are using the smaller `laxflights2022` data set described on the [data preparation](../../data-prep.qmd) page.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights <- readr::read_csv(here::here(\"data/laxflights2022_lite.csv\"))\n\nglimpse(flights)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,757\nColumns: 8\n$ arr_delay <dbl> 4, -15, -12, 38, -9, -17, 5, 12, -40, 6, -7, 28, 25, -9, 180…\n$ dep_delay <dbl> 9, -8, 0, -7, 3, 6, 29, -1, 2, 7, 6, 13, 34, -2, 191, 52, 9,…\n$ carrier   <chr> \"UA\", \"OO\", \"AA\", \"UA\", \"OO\", \"OO\", \"UA\", \"AA\", \"DL\", \"DL\", …\n$ tailnum   <chr> \"N37502\", \"N198SY\", \"N410AN\", \"N77261\", \"N402SY\", \"N509SY\", …\n$ origin    <chr> \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      <chr> \"KOA\", \"EUG\", \"HNL\", \"DEN\", \"FAT\", \"SFO\", \"MCO\", \"MIA\", \"OGG…\n$ distance  <dbl> 2504, 748, 2556, 862, 209, 337, 2218, 2342, 2486, 862, 156, …\n$ time      <dttm> 2022-01-01 13:15:00, 2022-01-01 14:00:00, 2022-01-01 14:45:…\n```\n\n\n:::\n:::\n## Modeling\n\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\n> Given all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay `arr_delay`?\n\nOur outcome is `arr_delay` and the remaining variables are predictors. We will be fitting an xgboost model as a regression model.\n\n### Splitting Data\n\nSince the data set is already in chronological order, we can create a time split of the data using `initial_time_split()`, this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set. \n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nflights_split <- initial_time_split(flights, prop = 3/4)\nflights_training <- training(flights_split)\n```\n:::\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_folds <- vfold_cv(flights_training)\n```\n:::\n\n### Feature Engineering\n\nWe need to do a couple of things to make this data set work for our model. The datetime variable `time` needs to be transformed, as does the categorical variables `carrier`, `tailnum`, `origin` and `dest`.\n\nFrom the `time` variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The `origin` and `dest` variables will be turned into dummy variables, and `carrier`, `tailnum`, `time_month`, and `time_dow` will be converted to numerics with likelihood encoding.\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_rec <- recipe(arr_delay ~ ., data = flights_training) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_other(origin, dest, threshold = 0.025) %>%\n  step_dummy(origin, dest) %>%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %>%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %>%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %>%\n  step_zv(all_predictors())\n```\n:::\n\n### Specifying Models\n\nWe will be fitting a boosted tree model in the form of a [xgboost model](https://parsnip.tidymodels.org/reference/details_boost_tree_xgboost.html).\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_spec <-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_wf <- workflow(flights_rec, xgb_spec)\n```\n:::\n\n### Hyperparameter Tuning\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\nxgb_rs <- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n\n\n:::\n:::\n\nWe can visualize the performance of the different hyperparameter selections\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(xgb_rs)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\nand look at the top result\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(xgb_rs, metric = \"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     3  1988     4 rmse    standard    28.1    10    5.88 Preprocessor1_Model01\n2     8   849    13 rmse    standard    29.6    10    6.44 Preprocessor1_Model04\n3     3  1543     9 rmse    standard    29.6    10    6.11 Preprocessor1_Model02\n4    10  1139    14 rmse    standard    30.0    10    6.44 Preprocessor1_Model05\n5    12   554    18 rmse    standard    30.6    10    6.77 Preprocessor1_Model06\n```\n\n\n:::\n:::\n\n### Fitting Final Model\n\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use `finalize_workflow()` to use the best hyperparameters, and `last_fit()` to fit the model to the training data set and evaluate it on the testing data set.\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_last <- xgb_wf %>%\n  finalize_workflow(select_best(xgb_rs, metric = \"rmse\")) %>%\n  last_fit(flights_split)\n```\n:::\n\n\n\n## Creating vetiver model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nv <- xgb_last %>%\n  extract_workflow() %>%\n  vetiver_model(\"flights_xgb\")\nv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n── flights_xgb ─ <bundled_workflow> model for deployment \nA xgboost regression modeling workflow using 7 features\n```\n\n\n:::\n:::\n\n\n## Version model with pins on Amazon SageMaker\n\nWe will version this model on [Amazon S3](https://aws.amazon.com/s3/) using the [pins](https://pins.rstudio.com/) package.\n\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.\n\n::: {.callout-warning}\nDepending on your S3 setup, you will need to use additional variables to connect. Please see <https://github.com/paws-r/paws/blob/main/docs/credentials.md> and this [pins issue](https://github.com/rstudio/pins-r/issues/608) for help if the following paragraphs doesn't work for you.\n:::\n\n::: {.callout-tip}\nThe function [usethis::edit_r_environ()](https://usethis.r-lib.org/reference/edit.html) can be very handy to open `.Renviron` file to specify your environment variables.\n:::\n\nYou can find both of these keys in the same location. \n\n1. Open the [AWS Console](https://console.aws.amazon.com/)\n1. Click on your username near the top right and select `Security Credentials`\n1. Click on `Users` in the sidebar\n1. Click on your username\n1. Click on the `Security Credentials` tab\n1. Click `Create Access Key`\n1. Click `Show User Security Credentials`\n\nOnce you have those two, you can add them to your `.Renviron` file in the following format:\n\n```markdown\nAWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAWS_ACCESS_KEY_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\nNote that you don't want to put quotes around the values.\n\n\n\nOnce that is all done, we can create a board that connects to Amazon S3, and write our vetiver model to the board. Now that you have set up the environment variables, we can create a pins board. When using S3 you need to specify a bucket and its region. This cannot be done with Pins and has to be done beforehand.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboard <- board_s3(\n  \"tidymodels-pipeline-example\",\n  region = \"us-west-1\"\n)\nvetiver_pin_write(board, v)\n```\n:::\n\n\nSince we are using `vetiver_deploy_sagemaker()` which uses the {smdocker} package, we need to make sure that we have the right authetication and settings.\n\nIf you are working locally, you will likely need to explicitly set up your execution role to work correctly. Check out [Execution role requirements](https://dyfanjones.r-universe.dev/smdocker) in the smdocker documentation, and especially note that the bucket containing your vetiver model needs to be added as a resource in your IAM role policy.\n\nOnce we are properly set up, we can use `vetiver_deploy_sagemaker()`, it takes a `board`, the name of endpoint and the `instance_type` Look at the [Amazon SageMaker pricing](https://aws.amazon.com/sagemaker/pricing/) to help you decide what you need. Depending on your model, it will take a little while to run as it installs what it needs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_endpoint <- vetiver_deploy_sagemaker(\n  board = board,\n  name = \"flights_xgb\",\n  instance_type = \"ml.t2.medium\"\n)\n```\n:::\n\n\n## Make predictions from Connect endpoint\n\nWith the endpoint we can pass in some data set to predict with.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(\n  new_endpoint,\n  flights_training\n)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,817 × 1\n    .pred\n    <dbl>\n 1  -1.79\n 2 -13.3 \n 3 -17.3 \n 4  -3.67\n 5  82.8 \n 6  52.2 \n 7  10.7 \n 8   8.04\n 9  58.1 \n10   5.39\n# ℹ 2,807 more rows\n```\n\n\n:::\n:::\n\n\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}