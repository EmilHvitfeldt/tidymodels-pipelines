{
  "hash": "47c159d703d8b0c9cbbd8b4ca556c311",
  "result": {
    "markdown": "---\ntitle: Standard Modeling Pipeline\ndescription: \"This analysis is done using tidymodels R alone\"\ncategories:\n  - csv\n---\n\n\n## Loading packages\n\nWe are using the tidymodels package to do the modeling and [embed](https://embed.tidymodels.org/) for target encoding.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"pak\")\n# pak::pak(\"tidymodels\", \"embed\")\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.5     ✔ recipes      1.0.9\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.3.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.3.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n```\n:::\n\n```{.r .cell-code}\nlibrary(embed)\n```\n:::\n\n\n## Loading Data\n\nWe are using the standard `laxflights2022` data set described on the [data preparation](../../data-prep.qmd) page.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights <- readr::read_csv(here::here(\"data/laxflights2022.csv\"))\n\nglimpse(flights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 187,868\nColumns: 8\n$ arr_delay <dbl> -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ dep_delay <dbl> 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ carrier   <chr> \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   <chr> \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    <chr> \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      <chr> \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  <dbl> 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      <dttm> 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n```\n:::\n:::\n\n\ndownsampling for speed for now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights <- slice_sample(flights, prop = 0.2) %>%\n  arrange(time)\n```\n:::\n\n\n## Modeling\n\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\n> Given all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay `arr_delay`.\n\nOur outcome is `arr_delay` and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\n### Splitting Data\n\nSince the data set is already in chronological order, we can create a time split of the data using `initial_time_split()`, this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nflights_split <- initial_time_split(flights, prop = 3/4)\nflights_training <- training(flights_split)\n```\n:::\n\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_folds <- vfold_cv(flights_training)\n```\n:::\n\n\n### Feature Engineering\n\nWe need to do a couple of things to make this data set work for our model. The datetime variable `time` needs to be transformed, as does the categorical variables `carrier`, `tailnum`, `origin` and `dest`.\n\nFrom the `time` variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The `origin` and `dest` variables will be turned into dummy variables, and `carrier`, `tailnum`, `time_month`, and `time_dow` will be converted to numerics with likelihood encoding.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_rec <- recipe(arr_delay ~ ., data = flights_training) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_other(origin, dest, threshold = 0.025) %>%\n  step_dummy(origin, dest) %>%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %>%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %>%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %>%\n  step_zv(all_predictors())\n```\n:::\n\n\n### Specifying Models\n\nWe will be fitting a boosted tree model in the form of a [xgboost model](https://parsnip.tidymodels.org/reference/details_boost_tree_xgboost.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_spec <-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_wf <- workflow(flights_rec, xgb_spec)\n```\n:::\n\n\n\n### Hyperparameter Tuning\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\nxgb_rs <- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n:::\n:::\n\n\nWe can visualize the performance of the different hyperparameter selections\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(xgb_rs)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nand look at the top result\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(xgb_rs, metric = \"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1    20  1134    12 rmse    standard    14.9    10    1.33 Preprocessor1_Model10\n2    17  1902    19 rmse    standard    15.5    10    1.54 Preprocessor1_Model08\n3    12   989     4 rmse    standard    16.2    10    1.23 Preprocessor1_Model06\n4    14   569    15 rmse    standard    16.3    10    1.58 Preprocessor1_Model07\n5    11  1302    27 rmse    standard    16.5    10    1.64 Preprocessor1_Model05\n```\n:::\n:::\n\n\n### Fitting Final Model\n\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use `finalize_workflow()` to use the best hyperparameters, and `last_fit()` to fit the model to the training data set and evaluate it on the testing data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_last <- xgb_wf %>%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %>%\n  last_fit(flights_split)\n```\n:::\n\n\n## Results\n\nOnce we have the final model, we can do all kind of analyses. Below we have a truth version prediction plot to showcase how well our model works\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_last %>%\n  augment() %>%\n  ggplot(aes(arr_delay, .pred)) +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\") +\n  geom_point(alpha = 0.25) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Truth against prediction plot. The model has a hard time with overly long\ndelays.\n](index_files/figure-html/unnamed-chunk-13-1.png){fig-alt='Scatter chart. `arr_delay` along the x-axis and `.pred` along the y-axis.\nA blue line is going diagonally with intercept 0 and slope 1. Most of the\npoints lie closely towards the line except for a handful of points with\nvery right delay values.' width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}