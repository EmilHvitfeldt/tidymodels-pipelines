{
  "hash": "b5535c5065a6de309ff39d98075500e5",
  "result": {
    "markdown": "---\ntitle: Standard Modeling Pipeline\ndescription: \"This analysis is done using tidymodels R alone\"\ncategories:\n  - csv\n  - vetiver\n  - docker\neval: false\n---\n\n\n::: new\n::: {.callout-note}\n## Changes from standard\n\nAll changes from the [standard pipeline](../standard/index.qmd) is highlighted with a cranberry line to the right.\n:::\n:::\n\n## Loading packages\n\nWe are using the tidymodels package to do the modeling and [embed](https://embed.tidymodels.org/) for target encoding.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(embed)\n```\n:::\n\n\n::: new\n\n## Loading Data\n\nWe are using the standard `laxflights2022` data set described on the [data preparation](../../data-prep.qmd) page.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights <- readr::read_csv(here::here(\"data/laxflights2022.csv\"))\n\nglimpse(flights)\n```\n:::\n\n\ndownsampling for speed for now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights <- slice_sample(flights, prop = 0.2) %>%\n  arrange(time)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n:::\n\n## Modeling\n\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\n> Given all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay `arr_delay`.\n\nOur outcome is `arr_delay` and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\n### Splitting Data\n\nSince the data set is already in chronological order, we can create a time split of the data using `initial_time_split()`, this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nflights_split <- initial_time_split(flights, prop = 3/4)\nflights_training <- training(flights_split)\n```\n:::\n\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_folds <- vfold_cv(flights_training)\n```\n:::\n\n\n### Feature Engineering\n\nWe need to do a couple of things to make this data set work for our model. The datetime variable `time` needs to be transformed, as does the categorical variables `carrier`, `tailnum`, `origin` and `dest`.\n\nFrom the `time` variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The `origin` and `dest` variables will be turned into dummy variables, and `carrier`, `tailnum`, `time_month`, and `time_dow` will be converted to numerics with likelihood encoding.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_rec <- recipe(arr_delay ~ ., data = flights_training) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_other(origin, dest, threshold = 0.025) %>%\n  step_dummy(origin, dest) %>%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %>%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %>%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %>%\n  step_zv(all_predictors())\n```\n:::\n\n\n### Specifying Models\n\nWe will be fitting a boosted tree model in the form of a [xgboost model](https://parsnip.tidymodels.org/reference/details_boost_tree_xgboost.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_spec <-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_wf <- workflow(flights_rec, xgb_spec)\n```\n:::\n\n\n\n### Hyperparameter Tuning\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\nxgb_rs <- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n```\n:::\n\n\nWe can visualize the performance of the different hyperparameter selections\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(xgb_rs)\n```\n:::\n\n\nand look at the top result\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(xgb_rs, metric = \"rmse\")\n```\n:::\n\n\n### Fitting Final Model\n\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use `finalize_workflow()` to use the best hyperparameters, and `last_fit()` to fit the model to the training data set and evaluate it on the testing data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_last <- xgb_wf %>%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %>%\n  last_fit(flights_split)\n```\n:::\n\n\n## Results\n\nOnce we have the final model, we can do all kind of analyses. Below we have a truth version prediction plot to showcase how well our model works\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_last %>%\n  augment() %>%\n  ggplot(aes(arr_delay, .pred)) +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\") +\n  geom_point(alpha = 0.25) +\n  theme_minimal()\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}