[
  {
    "objectID": "pipelines.html",
    "href": "pipelines.html",
    "title": "",
    "section": "",
    "text": "Pins, Vetiver and Amazon SageMaker\n\n\n\n\n\n\npins\n\n\nvetiver\n\n\nAmazon S3\n\n\nAmazon SageMaker\n\n\n\nUsing vetiver to version a model on Amazon SageMaker as an endpoint, and predict from it\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPins, Vetiver, S3, and Docker\n\n\n\n\n\n\npins\n\n\nvetiver\n\n\nDocker\n\n\nAmazon S3\n\n\n\nUsing vetiver to version a model in a S3 bucket, and predict with it from a Docker container\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Modeling Pipeline\n\n\n\n\n\n\ncsv\n\n\n\nThis analysis is done using tidymodels R alone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVetiver and Amazon SageMaker\n\n\n\n\n\n\ncsv\n\n\nvetiver\n\n\nAmazon SageMaker\n\n\n\nUsing vetiver to version a model on Amazon SageMaker as an endpoint, and predict from it\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVetiver and Posit Connect\n\n\n\n\n\n\ncsv\n\n\nvetiver\n\n\nPosit Connect\n\n\n\nUsing vetiver to version a model on Posit Connect as an endpoint, and predict from it\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVetiver and Posit Connect\n\n\n\n\n\n\npins\n\n\nvetiver\n\n\nPosit Connect\n\n\n\nUsing vetiver to version a model on Posit Connect as an endpoint, and predict from it\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVetiver, Azure, and Docker\n\n\n\n\n\n\ncsv\n\n\nvetiver\n\n\nDocker\n\n\nAzure\n\n\n\nUsing vetiver to version a model in a Azure Storage Container, and predict with it from a Docker container\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVetiver, Posit Connect, and Docker\n\n\n\n\n\n\ncsv\n\n\nvetiver\n\n\nDocker\n\n\nPosit Connect\n\n\n\nUsing vetiver to version a model on Posit Connect, and predict with it from a Docker container\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVetiver, S3, and Docker\n\n\n\n\n\n\ncsv\n\n\nvetiver\n\n\nDocker\n\n\nAmazon S3\n\n\n\nUsing vetiver to version a model in a S3 bucket, and predict with it from a Docker container\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data-pins.html",
    "href": "data-pins.html",
    "title": "Put data places with {pins}",
    "section": "",
    "text": "This page shows how to use pins to store data in various platforms as needed for the pipelines."
  },
  {
    "objectID": "data-pins.html#posit-connect",
    "href": "data-pins.html#posit-connect",
    "title": "Put data places with {pins}",
    "section": "Posit Connect",
    "text": "Posit Connect\nThis section goes through how to put data on Posit Connect using pins.\n\nConnecting to Posit Connect\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are CONNECT_SERVER and CONNECT_API_KEY.\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nCONNECT_SERVER is the URL of the posit connect page. So if your connect server is accessed through https://example.com/connect/#/content/ then you can find CONNECT_SERVER by removing connect/ and everything that follows it, leaving you with https://example.com/.\nCONNECT_API_KEY is created through your Connect server. 1. Click on your name in the upper right upper right. 1. Click API keys. 1. Click New API Key. 1. Give your API a key, click `Create Key.\nOnce you have those two, you can add them to your .Renviron file in the following format:\nCONNECT_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nCONNECT_SERVER=https://example.com/\nNote that you don’t want to put quotes around the values.\n\n\nCreating the board\n\nlibrary(pins)\n\nboard &lt;- board_connect()\n\nWe load in the data we created in data-prep\n\nflights &lt;- readr::read_csv(\"data/laxflights2022_lite.csv\")\n\n\n\nPinning data set to Connect\nNow that we have the data set, we can send it to Posit Connect using the board we created, the data set and a name of out choosing.\n\nboard |&gt;\n  pin_write(flights, name = \"laxflights2022_lite\", type = \"rds\")"
  },
  {
    "objectID": "data-pins.html#amazon-s3",
    "href": "data-pins.html#amazon-s3",
    "title": "Put data places with {pins}",
    "section": "Amazon S3",
    "text": "Amazon S3\nThis section goes through how to put data on Amazon S3 using pins.\n\nConnecting to Amazon S3\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.\n\n\n\n\n\n\nWarning\n\n\n\nDepending on your S3 setup, you will need to use additional variables to connect. Please see https://github.com/paws-r/paws/blob/main/docs/credentials.md and this pins issue for help if the following paragraphs doesn’t work for you.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nYou can find both of these keys in the same location.\n\nOpen the AWS Console\nClick on your username near the top right and select Security Credentials\nClick on Users in the sidebar\nClick on your username\nClick on the Security Credentials tab\nClick Create Access Key\nClick Show User Security Credentials\n\nOnce you have those two, you can add them to your .Renviron file in the following format:\nAWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAWS_ACCESS_KEY_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nNote that you don’t want to put quotes around the values.\n\n\nCreating the board\n\nlibrary(pins)\n\nboard &lt;- board_s3(\n  \"tidymodels-pipeline-example\",\n  region = \"us-west-1\"\n)\n\nWe load in the data we created in data-prep\n\nflights &lt;- readr::read_csv(\"data/laxflights2022_lite.csv\")\n\n\n\nPinning data set to Amazon S3\nNow that we have the data set, we can send it to Amazon S3 using the board we created, the data set and a name of out choosing.\n\nboard |&gt;\n  pin_write(flights, name = \"laxflights2022_lite\", type = \"rds\")"
  },
  {
    "objectID": "pipelines/standard/index.html",
    "href": "pipelines/standard/index.html",
    "title": "Standard Modeling Pipeline",
    "section": "",
    "text": "Note\n\n\n\nThis page was last generated on 2024-03-06. If you find the code out of date please file an issue."
  },
  {
    "objectID": "pipelines/standard/index.html#loading-packages",
    "href": "pipelines/standard/index.html#loading-packages",
    "title": "Standard Modeling Pipeline",
    "section": "Loading packages",
    "text": "Loading packages\nWe are using the tidymodels package to do the modeling and embed for target encoding.\n\n# install.packages(\"pak\")\n# pak::pak(\"tidymodels\", \"embed\")\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5      ✔ recipes      1.0.10\n✔ dials        1.2.0      ✔ rsample      1.2.0 \n✔ dplyr        1.1.4      ✔ tibble       3.2.1 \n✔ ggplot2      3.4.4      ✔ tidyr        1.3.1 \n✔ infer        1.0.6      ✔ tune         1.1.2 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.0      ✔ workflowsets 1.0.1 \n✔ purrr        1.0.2      ✔ yardstick    1.3.0 \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(embed)"
  },
  {
    "objectID": "pipelines/standard/index.html#loading-data",
    "href": "pipelines/standard/index.html#loading-data",
    "title": "Standard Modeling Pipeline",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the standard laxflights2022 data set described on the data preparation page.\n\nflights &lt;- readr::read_csv(here::here(\"data/laxflights2022.csv\"))\n\nglimpse(flights)\n\nRows: 187,868\nColumns: 8\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\ndownsampling for speed for now.\n\nflights &lt;- slice_sample(flights, prop = 0.2) %&gt;%\n  arrange(time)"
  },
  {
    "objectID": "pipelines/standard/index.html#modeling",
    "href": "pipelines/standard/index.html#modeling",
    "title": "Standard Modeling Pipeline",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    21  1134    12 rmse    standard    15.6    10    1.65 Preprocessor1_Model10\n2    17  1902    19 rmse    standard    16.4    10    1.89 Preprocessor1_Model08\n3    12   989     4 rmse    standard    16.4    10    1.38 Preprocessor1_Model06\n4    15   569    15 rmse    standard    16.8    10    1.80 Preprocessor1_Model07\n5    11  1302    27 rmse    standard    17.2    10    1.87 Preprocessor1_Model05\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)"
  },
  {
    "objectID": "pipelines/standard/index.html#results",
    "href": "pipelines/standard/index.html#results",
    "title": "Standard Modeling Pipeline",
    "section": "Results",
    "text": "Results\nOnce we have the final model, we can do all kind of analyses. Below we have a truth version prediction plot to showcase how well our model works\n\nxgb_last %&gt;%\n  augment() %&gt;%\n  ggplot(aes(arr_delay, .pred)) +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\") +\n  geom_point(alpha = 0.25) +\n  theme_minimal()\n\n\n\n\nTruth against prediction plot. The model has a hard time with overly long delays."
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html",
    "href": "pipelines/csv-connect-docker/index.html",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "",
    "text": "Note\n\n\n\nThis page was last generated on 2024-03-01. If you find the code out of date please file an issue."
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#loading-packages",
    "href": "pipelines/csv-connect-docker/index.html#loading-packages",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Loading packages",
    "text": "Loading packages\nWe are using the tidymodels package to do the modeling, embed for target encoding, pins for versioning, and vetiver for version and deployment.\n\n# install.packages(\"pak\")\n# pak::pak(\"tidymodels\", \"embed\", \"vetiver\", \"pins\")\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(vetiver)\nlibrary(pins)"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#loading-data",
    "href": "pipelines/csv-connect-docker/index.html#loading-data",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the smaller laxflights2022 data set described on the data preparation page.\n\nflights &lt;- readr::read_csv(here::here(\"data/laxflights2022_lite.csv\"))\n\nglimpse(flights)\n\nRows: 3,757\nColumns: 8\n$ arr_delay &lt;dbl&gt; 4, -15, -12, 38, -9, -17, 5, 12, -40, 6, -7, 28, 25, -9, 180…\n$ dep_delay &lt;dbl&gt; 9, -8, 0, -7, 3, 6, 29, -1, 2, 7, 6, 13, 34, -2, 191, 52, 9,…\n$ carrier   &lt;chr&gt; \"UA\", \"OO\", \"AA\", \"UA\", \"OO\", \"OO\", \"UA\", \"AA\", \"DL\", \"DL\", …\n$ tailnum   &lt;chr&gt; \"N37502\", \"N198SY\", \"N410AN\", \"N77261\", \"N402SY\", \"N509SY\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"KOA\", \"EUG\", \"HNL\", \"DEN\", \"FAT\", \"SFO\", \"MCO\", \"MIA\", \"OGG…\n$ distance  &lt;dbl&gt; 2504, 748, 2556, 862, 209, 337, 2218, 2342, 2486, 862, 156, …\n$ time      &lt;dttm&gt; 2022-01-01 13:15:00, 2022-01-01 14:00:00, 2022-01-01 14:45:…"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#modeling",
    "href": "pipelines/csv-connect-docker/index.html#modeling",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3  1988     4 rmse    standard    28.1    10    5.88 Preprocessor1_Model01\n2     8   849    13 rmse    standard    29.6    10    6.44 Preprocessor1_Model04\n3     3  1543     9 rmse    standard    29.6    10    6.11 Preprocessor1_Model02\n4    10  1139    14 rmse    standard    30.0    10    6.44 Preprocessor1_Model05\n5    12   554    18 rmse    standard    30.6    10    6.77 Preprocessor1_Model06\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#creating-vetiver-model",
    "href": "pipelines/csv-connect-docker/index.html#creating-vetiver-model",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Creating vetiver model",
    "text": "Creating vetiver model\n\nv &lt;- xgb_last %&gt;%\n  extract_workflow() %&gt;%\n  vetiver_model(\"flights_xgb\")\nv\n\n\n── flights_xgb ─ &lt;bundled_workflow&gt; model for deployment \nA xgboost regression modeling workflow using 7 features"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#version-model-with-pins-on-posit-connect",
    "href": "pipelines/csv-connect-docker/index.html#version-model-with-pins-on-posit-connect",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Version model with pins on Posit Connect",
    "text": "Version model with pins on Posit Connect\nWe will version this model on Posit Connect using the pins package.\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are CONNECT_SERVER and CONNECT_API_KEY.\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nCONNECT_SERVER is the URL of the posit connect page. So if your connect server is accessed through https://example.com/connect/#/content/ then you can find CONNECT_SERVER by removing connect/ and everything that follows it, leaving you with https://example.com/.\nCONNECT_API_KEY is created through your Connect server. 1. Click on your name in the upper right upper right. 1. Click API keys. 1. Click New API Key. 1. Give your API a key, click `Create Key.\nOnce you have those two, you can add them to your .Renviron file in the following format:\nCONNECT_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nCONNECT_SERVER=https://example.com/\nNote that you don’t want to put quotes around the values.\nOnce that is all done, we can create a board that connects to Connect, and write our vetiver model to the board.\n\nboard &lt;- board_connect()\nvetiver_pin_write(board, v)"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#create-docker-artifacts",
    "href": "pipelines/csv-connect-docker/index.html#create-docker-artifacts",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Create Docker artifacts",
    "text": "Create Docker artifacts\nTo build a Docker image that can serve your model, you need three artifacts:\n\nthe Dockerfile itself,\na renv.lock to capture your model dependencies, and\nan plumber.R file containing the information to serve a vetiver REST API.\n\nYou can create all the needed files with one function.\n\nvetiver_prepare_docker(\n  board, \n  \"emil.hvitfeldt/flights_xgb\", \n  docker_args = list(port = 8080)\n)\n\nThe following package(s) will be updated in the lockfile:\n\n# Local ----------------------------------------------------------------------\n- parsnip   [1.2.0 -&gt; 1.2.0.9001]\n- recipes   [1.0.10 -&gt; 1.0.9.9000]\n\n- Lockfile written to \"vetiver_renv.lock\".\n\n\n\n\n\n\n\n\nNote\n\n\n\nKeep an eye on the value of port, we want to make sure we use the same throughout the whole pipeline.\n\n\nFor ease of use, we make sure only to have CRAN versions of packages."
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#build-and-run-your-dockerfile",
    "href": "pipelines/csv-connect-docker/index.html#build-and-run-your-dockerfile",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Build and run your Dockerfile",
    "text": "Build and run your Dockerfile\nNow that we have everything we need to build a Docker image. We have one more thing to do. Install Docker if you haven’t already, then launch it so we can interact with it from the command line (not from R). Use the following docker build command. Notice that we can give it a “name” using the --tag flag. The . here denotes the path to the build context. Which in this example is the folder we are in.\ndocker build --tag flights .\n\n\n\n\n\n\nTip\n\n\n\nIf you are on an ARM architecture locally and deploying an R model, use --platform linux/amd64 for RSPM’s fast installation of R package binaries.\n\n\nTo run the docker container, we need to pass in the environment variables for the code to connect to the Connect server. We could pass in the system environment variables, but we will be safer if we just pass in what we need. We do this by creating a project-specific .Renviron file. (fs::file_touch(\".Renviron\")) and specifying CONNECT_SERVER and CONNECT_API_KEY in that file.\nThen we run docker run command. We set 2 flags, --env-file to pass in the environment variables we need, and --publish to specify the port.\ndocker run --env-file .Renviron --publish 8080:8080 flights"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#make-predictions-from-docker-container",
    "href": "pipelines/csv-connect-docker/index.html#make-predictions-from-docker-container",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Make predictions from Docker container",
    "text": "Make predictions from Docker container\nNow that the docker container is running we can create an endpoint with vetiver_endpoint(), and that endpoint can be used as a way to make predictions.\n\nendpoint &lt;- vetiver_endpoint(\"http://0.0.0.0:8080/predict\")\n\npredict(endpoint, flights_training)\n\n\n\n# A tibble: 2,817 × 1\n    .pred\n    &lt;dbl&gt;\n 1  -1.79\n 2 -13.3 \n 3 -17.3 \n 4  -3.67\n 5  82.8 \n 6  52.2 \n 7  10.7 \n 8   8.04\n 9  58.1 \n10   5.39\n# ℹ 2,807 more rows"
  },
  {
    "objectID": "pipelines/csv-sagemaker-deploy/index.html",
    "href": "pipelines/csv-sagemaker-deploy/index.html",
    "title": "Vetiver and Amazon SageMaker",
    "section": "",
    "text": "Note\n\n\n\nThis page was last generated on 2024-02-29. If you find the code out of date please file an issue."
  },
  {
    "objectID": "pipelines/csv-sagemaker-deploy/index.html#loading-packages",
    "href": "pipelines/csv-sagemaker-deploy/index.html#loading-packages",
    "title": "Vetiver and Amazon SageMaker",
    "section": "Loading packages",
    "text": "Loading packages\nWe are using the tidymodels package to do the modeling, embed for target encoding, pins for versioning, vetiver for version and deployment, and smdocker for deploying to SageMaker.\n\n# install.packages(\"pak\")\n# pak::pak(\"tidymodels\", \"embed\", \"vetiver\", \"pins\", \"smdocker\")\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(smdocker)"
  },
  {
    "objectID": "pipelines/csv-sagemaker-deploy/index.html#loading-data",
    "href": "pipelines/csv-sagemaker-deploy/index.html#loading-data",
    "title": "Vetiver and Amazon SageMaker",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the smaller laxflights2022 data set described on the data preparation page.\n\nflights &lt;- readr::read_csv(here::here(\"data/laxflights2022_lite.csv\"))\n\nglimpse(flights)\n\nRows: 3,757\nColumns: 8\n$ arr_delay &lt;dbl&gt; 4, -15, -12, 38, -9, -17, 5, 12, -40, 6, -7, 28, 25, -9, 180…\n$ dep_delay &lt;dbl&gt; 9, -8, 0, -7, 3, 6, 29, -1, 2, 7, 6, 13, 34, -2, 191, 52, 9,…\n$ carrier   &lt;chr&gt; \"UA\", \"OO\", \"AA\", \"UA\", \"OO\", \"OO\", \"UA\", \"AA\", \"DL\", \"DL\", …\n$ tailnum   &lt;chr&gt; \"N37502\", \"N198SY\", \"N410AN\", \"N77261\", \"N402SY\", \"N509SY\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"KOA\", \"EUG\", \"HNL\", \"DEN\", \"FAT\", \"SFO\", \"MCO\", \"MIA\", \"OGG…\n$ distance  &lt;dbl&gt; 2504, 748, 2556, 862, 209, 337, 2218, 2342, 2486, 862, 156, …\n$ time      &lt;dttm&gt; 2022-01-01 13:15:00, 2022-01-01 14:00:00, 2022-01-01 14:45:…"
  },
  {
    "objectID": "pipelines/csv-sagemaker-deploy/index.html#modeling",
    "href": "pipelines/csv-sagemaker-deploy/index.html#modeling",
    "title": "Vetiver and Amazon SageMaker",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3  1988     4 rmse    standard    28.1    10    5.88 Preprocessor1_Model01\n2     8   849    13 rmse    standard    29.6    10    6.44 Preprocessor1_Model04\n3     3  1543     9 rmse    standard    29.6    10    6.11 Preprocessor1_Model02\n4    10  1139    14 rmse    standard    30.0    10    6.44 Preprocessor1_Model05\n5    12   554    18 rmse    standard    30.6    10    6.77 Preprocessor1_Model06\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)"
  },
  {
    "objectID": "pipelines/csv-sagemaker-deploy/index.html#creating-vetiver-model",
    "href": "pipelines/csv-sagemaker-deploy/index.html#creating-vetiver-model",
    "title": "Vetiver and Amazon SageMaker",
    "section": "Creating vetiver model",
    "text": "Creating vetiver model\n\nv &lt;- xgb_last %&gt;%\n  extract_workflow() %&gt;%\n  vetiver_model(\"flights_xgb\")\nv\n\n\n── flights_xgb ─ &lt;bundled_workflow&gt; model for deployment \nA xgboost regression modeling workflow using 7 features"
  },
  {
    "objectID": "pipelines/csv-sagemaker-deploy/index.html#version-model-with-pins-on-amazon-sagemaker",
    "href": "pipelines/csv-sagemaker-deploy/index.html#version-model-with-pins-on-amazon-sagemaker",
    "title": "Vetiver and Amazon SageMaker",
    "section": "Version model with pins on Amazon SageMaker",
    "text": "Version model with pins on Amazon SageMaker\nWe will version this model on Amazon S3 using the pins package.\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.\n\n\n\n\n\n\nWarning\n\n\n\nDepending on your S3 setup, you will need to use additional variables to connect. Please see https://github.com/paws-r/paws/blob/main/docs/credentials.md and this pins issue for help if the following paragraphs doesn’t work for you.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nYou can find both of these keys in the same location.\n\nOpen the AWS Console\nClick on your username near the top right and select Security Credentials\nClick on Users in the sidebar\nClick on your username\nClick on the Security Credentials tab\nClick Create Access Key\nClick Show User Security Credentials\n\nOnce you have those two, you can add them to your .Renviron file in the following format:\nAWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAWS_ACCESS_KEY_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nNote that you don’t want to put quotes around the values.\nOnce that is all done, we can create a board that connects to Amazon S3, and write our vetiver model to the board. Now that you have set up the environment variables, we can create a pins board. When using S3 you need to specify a bucket and its region. This cannot be done with Pins and has to be done beforehand.\n\nboard &lt;- board_s3(\n  \"tidymodels-pipeline-example\",\n  region = \"us-west-1\"\n)\nvetiver_pin_write(board, v)\n\nSince we are using vetiver_deploy_sagemaker() which uses the {smdocker} package, we need to make sure that we have the right authetication and settings.\nIf you are working locally, you will likely need to explicitly set up your execution role to work correctly. Check out Execution role requirements in the smdocker documentation, and especially note that the bucket containing your vetiver model needs to be added as a resource in your IAM role policy.\nOnce we are properly set up, we can use vetiver_deploy_sagemaker(), it takes a board, the name of endpoint and the instance_type Look at the Amazon SageMaker pricing to help you decide what you need. Depending on your model, it will take a little while to run as it installs what it needs.\n\nnew_endpoint &lt;- vetiver_deploy_sagemaker(\n  board = board,\n  name = \"flights_xgb\",\n  instance_type = \"ml.t2.medium\"\n)"
  },
  {
    "objectID": "pipelines/csv-sagemaker-deploy/index.html#make-predictions-from-connect-endpoint",
    "href": "pipelines/csv-sagemaker-deploy/index.html#make-predictions-from-connect-endpoint",
    "title": "Vetiver and Amazon SageMaker",
    "section": "Make predictions from Connect endpoint",
    "text": "Make predictions from Connect endpoint\nWith the endpoint we can pass in some data set to predict with.\n\npredict(\n  new_endpoint,\n  flights_training\n)\n\n\n\n# A tibble: 2,817 × 1\n    .pred\n    &lt;dbl&gt;\n 1  -1.79\n 2 -13.3 \n 3 -17.3 \n 4  -3.67\n 5  82.8 \n 6  52.2 \n 7  10.7 \n 8   8.04\n 9  58.1 \n10   5.39\n# ℹ 2,807 more rows"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html",
    "href": "pipelines/csv-s3-docker/index.html",
    "title": "Vetiver, S3, and Docker",
    "section": "",
    "text": "Note\n\n\n\nThis page was last generated on 2023-12-15. If you find the code out of date please file an issue."
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#loading-packages",
    "href": "pipelines/csv-s3-docker/index.html#loading-packages",
    "title": "Vetiver, S3, and Docker",
    "section": "Loading packages",
    "text": "Loading packages\nWe are using the tidymodels package to do the modeling, embed for target encoding, pins for versioning, vetiver for version and deployment, and paws.storage for S3 connections.\n\n# install.packages(\"pak\")\n# pak::pak(\"tidymodels\", \"embed\", \"vetiver\", \"pins\", \"paws.storage\")\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(paws.storage)"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#loading-data",
    "href": "pipelines/csv-s3-docker/index.html#loading-data",
    "title": "Vetiver, S3, and Docker",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the smaller laxflights2022 data set described on the data preparation page.\n\nflights &lt;- readr::read_csv(here::here(\"data/laxflights2022_lite.csv\"))\n\nglimpse(flights)\n\nRows: 3,757\nColumns: 8\n$ arr_delay &lt;dbl&gt; 4, -15, -12, 38, -9, -17, 5, 12, -40, 6, -7, 28, 25, -9, 180…\n$ dep_delay &lt;dbl&gt; 9, -8, 0, -7, 3, 6, 29, -1, 2, 7, 6, 13, 34, -2, 191, 52, 9,…\n$ carrier   &lt;chr&gt; \"UA\", \"OO\", \"AA\", \"UA\", \"OO\", \"OO\", \"UA\", \"AA\", \"DL\", \"DL\", …\n$ tailnum   &lt;chr&gt; \"N37502\", \"N198SY\", \"N410AN\", \"N77261\", \"N402SY\", \"N509SY\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"KOA\", \"EUG\", \"HNL\", \"DEN\", \"FAT\", \"SFO\", \"MCO\", \"MIA\", \"OGG…\n$ distance  &lt;dbl&gt; 2504, 748, 2556, 862, 209, 337, 2218, 2342, 2486, 862, 156, …\n$ time      &lt;dttm&gt; 2022-01-01 13:15:00, 2022-01-01 14:00:00, 2022-01-01 14:45:…"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#modeling",
    "href": "pipelines/csv-s3-docker/index.html#modeling",
    "title": "Vetiver, S3, and Docker",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3  1988     4 rmse    standard    28.1    10    5.88 Preprocessor1_Model01\n2     8   849    13 rmse    standard    29.6    10    6.44 Preprocessor1_Model04\n3     3  1543     9 rmse    standard    29.6    10    6.11 Preprocessor1_Model02\n4    10  1139    14 rmse    standard    30.0    10    6.44 Preprocessor1_Model05\n5    12   554    18 rmse    standard    30.6    10    6.77 Preprocessor1_Model06\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#creating-vetiver-model",
    "href": "pipelines/csv-s3-docker/index.html#creating-vetiver-model",
    "title": "Vetiver, S3, and Docker",
    "section": "Creating vetiver model",
    "text": "Creating vetiver model\n\nv &lt;- xgb_last %&gt;%\n  extract_workflow() %&gt;%\n  vetiver_model(\"flights_xgb\")\nv\n\n\n── flights_xgb ─ &lt;bundled_workflow&gt; model for deployment \nA xgboost regression modeling workflow using 7 features"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#version-model-with-pins-on-amazon-s3",
    "href": "pipelines/csv-s3-docker/index.html#version-model-with-pins-on-amazon-s3",
    "title": "Vetiver, S3, and Docker",
    "section": "Version model with pins on Amazon S3",
    "text": "Version model with pins on Amazon S3\nWe will version this model on Amazon S3 using the pins package.\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.\n\n\n\n\n\n\nWarning\n\n\n\nDepending on your S3 setup, you will need to use additional variables to connect. Please see https://github.com/paws-r/paws/blob/main/docs/credentials.md and this pins issue for help if the following paragraphs doesn’t work for you.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nYou can find both of these keys in the same location.\n\nOpen the AWS Console\nClick on your username near the top right and select Security Credentials\nClick on Users in the sidebar\nClick on your username\nClick on the Security Credentials tab\nClick Create Access Key\nClick Show User Security Credentials\n\nOnce you have those two, you can add them to your .Renviron file in the following format:\nAWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAWS_ACCESS_KEY_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nNote that you don’t want to put quotes around the values.\nOnce that is all done, we can create a board that connects to Amazon S3, and write our vetiver model to the board. Now that you have set up the environment variables, we can create a pins board. When using S3 you need to specify a bucket and its region. This cannot be done with Pins and has to be done beforehand.\n\nboard &lt;- board_s3(\n  \"tidymodels-pipeline-example\",\n  region = \"us-west-1\"\n)\nvetiver_pin_write(board, v)"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#create-docker-artifacts",
    "href": "pipelines/csv-s3-docker/index.html#create-docker-artifacts",
    "title": "Vetiver, S3, and Docker",
    "section": "Create Docker artifacts",
    "text": "Create Docker artifacts\nTo build a Docker image that can serve your model, you need three artifacts:\n\nthe Dockerfile itself,\na renv.lock to capture your model dependencies, and\nan plumber.R file containing the information to serve a vetiver REST API.\n\nYou can create all the needed files with one function.\n\nvetiver_prepare_docker(\n  board, \n  \"flights_xgb\", \n  docker_args = list(port = 8080)\n)\n\nThe following package(s) will be updated in the lockfile:\n\n# Local ----------------------------------------------------------------------\n- parsnip   [1.2.0 -&gt; 1.2.0.9001]\n- recipes   [1.0.10 -&gt; 1.0.9.9000]\n\n- Lockfile written to \"vetiver_renv.lock\".\n\n\n\n\n\n\n\n\nNote\n\n\n\nKeep an eye on the value of port, we want to make sure we use the same throughout the whole pipeline.\n\n\nFor ease of use, we make sure only to have CRAN versions of packages."
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#build-and-run-your-dockerfile",
    "href": "pipelines/csv-s3-docker/index.html#build-and-run-your-dockerfile",
    "title": "Vetiver, S3, and Docker",
    "section": "Build and run your Dockerfile",
    "text": "Build and run your Dockerfile\nNow that we have everything we need to build a Docker image. We have one more thing to do. Install Docker if you haven’t already, then launch it so we can interact with it from the command line (not from R). Use the following docker build command. Notice that we can give it a “name” using the --tag flag. The . here denotes the path to the build context. Which in this example is the folder we are in.\ndocker build --tag flights .\n\n\n\n\n\n\nTip\n\n\n\nIf you are on an ARM architecture locally and deploying an R model, use --platform linux/amd64 for RSPM’s fast installation of R package binaries.\n\n\nTo run the docker container, we need to pass in the environment variables for the code to connect to the Connect server. We could pass in the system environment variables, but we will be safer if we just pass in what we need. We do this by creating a project-specific .Renviron file. (fs::file_touch(\".Renviron\")) and specifying AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in that file.\nThen we run docker run command. We set 2 flags, --env-file to pass in the environment variables we need, and --publish to specify the port.\ndocker run --env-file .Renviron --publish 8080:8080 flights"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#make-predictions-from-docker-container",
    "href": "pipelines/csv-s3-docker/index.html#make-predictions-from-docker-container",
    "title": "Vetiver, S3, and Docker",
    "section": "Make predictions from Docker container",
    "text": "Make predictions from Docker container\nNow that the docker container is running we can create an endpoint with vetiver_endpoint(), and that endpoint can be used as a way to make predictions.\n\nendpoint &lt;- vetiver_endpoint(\"http://0.0.0.0:8080/predict\")\n\npredict(endpoint, flights_training)\n\n\n\n# A tibble: 2,817 × 1\n    .pred\n    &lt;dbl&gt;\n 1  -1.79\n 2 -13.3 \n 3 -17.3 \n 4  -3.67\n 5  82.8 \n 6  52.2 \n 7  10.7 \n 8   8.04\n 9  58.1 \n10   5.39\n# ℹ 2,807 more rows"
  },
  {
    "objectID": "data-prep.html",
    "href": "data-prep.html",
    "title": "Data Preparation",
    "section": "",
    "text": "This page goes through how we generate the data set laxflights2022 that is used throughout this project."
  },
  {
    "objectID": "data-prep.html#loading-packages",
    "href": "data-prep.html#loading-packages",
    "title": "Data Preparation",
    "section": "Loading packages",
    "text": "Loading packages\nWe load tidyverse for general data manipulation and anyflights to download the data.\n\nlibrary(tidyverse)\nlibrary(anyflights)"
  },
  {
    "objectID": "data-prep.html#downloading-the-data",
    "href": "data-prep.html#downloading-the-data",
    "title": "Data Preparation",
    "section": "Downloading the data",
    "text": "Downloading the data\nThe data set we are using can be downloaded with a single call to get_flights()\n\nlaxflights2022raw &lt;- get_flights(\"LAX\", year = 2022, months = 1:12)\n\nTo avoid repeated downloads, we save the data set right after we download it. Can be found at data-raw/laxflights2022raw.csv.\n\nwrite_csv(laxflights2022raw, \"data-raw/laxflights2022raw.csv\")\n\n\n\n\n\n\n\nBad internet connection\n\n\n\nIf you are having issues with downloading this data all at once then you can split up the download into smaller chunks like so:\nlaxflights1 &lt;- anyflights(\"LAX\", 2022, 1:6)\nlaxflights2 &lt;- anyflights(\"LAX\", 2022, 7:12)\n\n\nlaxflights2022raw &lt;- dplyr::bind_rows(\n  laxflights1$flights,\n  laxflights2$flights\n)"
  },
  {
    "objectID": "data-prep.html#cleaning-the-data",
    "href": "data-prep.html#cleaning-the-data",
    "title": "Data Preparation",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nWe will do two things to this data set:\n\nremove redundant variables\nremove variables that won’t be available at the prediction time\n\nWhen looking at the data\n\nglimpse(laxflights2022raw)\n\nRows: 191,156\nColumns: 19\n$ year           &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2…\n$ month          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;dbl&gt; 7, 14, 15, 23, 25, 31, 34, 109, 119, 122, 135, 139, 224…\n$ sched_dep_time &lt;dbl&gt; 2359, 2343, 2315, 30, 2259, 2312, 25, 59, 55, 50, 2340,…\n$ dep_delay      &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, …\n$ arr_time       &lt;dbl&gt; 459, 608, 733, 742, 745, 745, 611, 608, 643, 608, 1000,…\n$ sched_arr_time &lt;dbl&gt; 511, 540, 647, 820, 631, 636, 631, 615, 633, 552, 811, …\n$ arr_delay      &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 12…\n$ carrier        &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"…\n$ flight         &lt;dbl&gt; 468, 359, 44, 177, 185, 205, 600, 122, 317, 33, 276, 59…\n$ tailnum        &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509…\n$ origin         &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\",…\n$ dest           &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\",…\n$ air_time       &lt;dbl&gt; 153, 203, 234, 238, 240, 231, 187, 160, 177, 143, 298, …\n$ distance       &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1…\n$ hour           &lt;dbl&gt; 23, 23, 23, 0, 22, 23, 0, 0, 0, 0, 23, 1, 23, 5, 5, 5, …\n$ minute         &lt;dbl&gt; 59, 43, 15, 30, 59, 12, 25, 59, 55, 50, 40, 30, 32, 10,…\n$ time_hour      &lt;dttm&gt; 2022-01-01 23:00:00, 2022-01-01 23:00:00, 2022-01-01 2…\n\n\nthe first thing we notice is that time_hour encodes almost the same information as year, month, day, hour and minute. With minute being the only difference. Let us deal with this by adding the minutes to time_hour and only keeping that variable\n\nlaxflights2022 &lt;- laxflights2022raw |&gt;\n  mutate(time = time_hour + minutes(minute)) |&gt;\n  select(-c(year, month, day, hour, minute, time_hour))\n\nglimpse(laxflights2022)\n\nRows: 191,156\nColumns: 14\n$ dep_time       &lt;dbl&gt; 7, 14, 15, 23, 25, 31, 34, 109, 119, 122, 135, 139, 224…\n$ sched_dep_time &lt;dbl&gt; 2359, 2343, 2315, 30, 2259, 2312, 25, 59, 55, 50, 2340,…\n$ dep_delay      &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, …\n$ arr_time       &lt;dbl&gt; 459, 608, 733, 742, 745, 745, 611, 608, 643, 608, 1000,…\n$ sched_arr_time &lt;dbl&gt; 511, 540, 647, 820, 631, 636, 631, 615, 633, 552, 811, …\n$ arr_delay      &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 12…\n$ carrier        &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"…\n$ flight         &lt;dbl&gt; 468, 359, 44, 177, 185, 205, 600, 122, 317, 33, 276, 59…\n$ tailnum        &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509…\n$ origin         &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\",…\n$ dest           &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\",…\n$ air_time       &lt;dbl&gt; 153, 203, 234, 238, 240, 231, 187, 160, 177, 143, 298, …\n$ distance       &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1…\n$ time           &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 2…\n\n\nNext, we see that the combination of dep_time, sched_dep_time and dep_delay is linearly dependent as dep_time - sched_dep_time = dep_delay, so we can remove sched_dep_time and dep_time without losing any information since we also have sched_dep_time as a function of time.\n\nlaxflights2022 &lt;- laxflights2022 |&gt;\n  select(-c(sched_dep_time, dep_time))\n\nglimpse(laxflights2022)\n\nRows: 191,156\nColumns: 12\n$ dep_delay      &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, …\n$ arr_time       &lt;dbl&gt; 459, 608, 733, 742, 745, 745, 611, 608, 643, 608, 1000,…\n$ sched_arr_time &lt;dbl&gt; 511, 540, 647, 820, 631, 636, 631, 615, 633, 552, 811, …\n$ arr_delay      &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 12…\n$ carrier        &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"…\n$ flight         &lt;dbl&gt; 468, 359, 44, 177, 185, 205, 600, 122, 317, 33, 276, 59…\n$ tailnum        &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509…\n$ origin         &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\",…\n$ dest           &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\",…\n$ air_time       &lt;dbl&gt; 153, 203, 234, 238, 240, 231, 187, 160, 177, 143, 298, …\n$ distance       &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1…\n$ time           &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 2…\n\n\nOn a similar note, since we are trying to predict arr_delay, we can’t have arr_time or air_time in the data set as they allow us to figure out arr_delay from dep_time. With some loss of information, we will also remove sched_arr_time to make the modeling a little easier.\n\nlaxflights2022 &lt;- laxflights2022 |&gt;\n  select(-c(arr_time, air_time, sched_arr_time))\n\nglimpse(laxflights2022)\n\nRows: 191,156\nColumns: 9\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ flight    &lt;dbl&gt; 468, 359, 44, 177, 185, 205, 600, 122, 317, 33, 276, 595, 51…\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\nWe will also be removing the flight variable to have one less high cardinality variable to work with.\n\nlaxflights2022 &lt;- laxflights2022 |&gt;\n  select(-flight)\n\nglimpse(laxflights2022)\n\nRows: 191,156\nColumns: 8\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\nSince we are trying to model arr_delay we exclude all the flights where that information is missing.\n\nlaxflights2022 &lt;- laxflights2022 |&gt;\n  drop_na(arr_delay)\n\nglimpse(laxflights2022)\n\nRows: 187,868\nColumns: 8\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\nLastly, the outcome arr_delay has been moved to the beginning of the data set as it aids in investigating the data.\n\nlaxflights2022 &lt;- laxflights2022 |&gt;\n  relocate(arr_delay)\n\nglimpse(laxflights2022)\n\nRows: 187,868\nColumns: 8\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…"
  },
  {
    "objectID": "data-prep.html#saving-the-data",
    "href": "data-prep.html#saving-the-data",
    "title": "Data Preparation",
    "section": "Saving the data",
    "text": "Saving the data\nThe full data set is now ready and is saved as data/laxflights2022.csv.\n\nwrite_csv(laxflights2022, \"data/laxflights2022.csv\")\n\nA smaller version of the data set is also created and is saved as data/laxflights2022_lite.csv.\n\nset.seed(1234)\nlaxflights2022_lite &lt;- laxflights2022 |&gt;\n  slice_sample(prop = 0.02) %&gt;%\n  arrange(time)\nwrite_csv(laxflights2022_lite, \"data/laxflights2022_lite.csv\")"
  },
  {
    "objectID": "data-prep.html#data-dictionary",
    "href": "data-prep.html#data-dictionary",
    "title": "Data Preparation",
    "section": "Data dictionary",
    "text": "Data dictionary\n\narr_delay: Arrival delays, in minutes. Negative times represent early arrivals.\ndep_delay: Departure delays, in minutes. Negative times represent early departures.\ncarrier: Two letter carrier abbreviation.\ntailnum: Plane tail number.\norigin: FAA airport code for origin of flight.\ndest: FAA airport code for destination of flight.\ndistance: Distance between airports, in miles.\ntime: Scheduled time of the flight as a POSIXct date, rounded to the nearest minute."
  },
  {
    "objectID": "pipelines/csv-connect-deploy/index.html",
    "href": "pipelines/csv-connect-deploy/index.html",
    "title": "Vetiver and Posit Connect",
    "section": "",
    "text": "Note\n\n\n\nThis page was last generated on 2024-03-08. If you find the code out of date please file an issue."
  },
  {
    "objectID": "pipelines/csv-connect-deploy/index.html#loading-packages",
    "href": "pipelines/csv-connect-deploy/index.html#loading-packages",
    "title": "Vetiver and Posit Connect",
    "section": "Loading packages",
    "text": "Loading packages\nWe are using the tidymodels package to do the modeling, embed for target encoding, pins for versioning, vetiver for version and deployment, and rsconnect for connecting with Posit Connect.\n\n# install.packages(\"pak\")\n# pak::pak(\"tidymodels\", \"embed\", \"vetiver\", \"pins\", \"rsconnect\")\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(rsconnect)"
  },
  {
    "objectID": "pipelines/csv-connect-deploy/index.html#loading-data",
    "href": "pipelines/csv-connect-deploy/index.html#loading-data",
    "title": "Vetiver and Posit Connect",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the smaller laxflights2022 data set described on the data preparation page.\n\nflights &lt;- readr::read_csv(here::here(\"data/laxflights2022_lite.csv\"))\n\nglimpse(flights)\n\nRows: 3,757\nColumns: 8\n$ arr_delay &lt;dbl&gt; 4, -15, -12, 38, -9, -17, 5, 12, -40, 6, -7, 28, 25, -9, 180…\n$ dep_delay &lt;dbl&gt; 9, -8, 0, -7, 3, 6, 29, -1, 2, 7, 6, 13, 34, -2, 191, 52, 9,…\n$ carrier   &lt;chr&gt; \"UA\", \"OO\", \"AA\", \"UA\", \"OO\", \"OO\", \"UA\", \"AA\", \"DL\", \"DL\", …\n$ tailnum   &lt;chr&gt; \"N37502\", \"N198SY\", \"N410AN\", \"N77261\", \"N402SY\", \"N509SY\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"KOA\", \"EUG\", \"HNL\", \"DEN\", \"FAT\", \"SFO\", \"MCO\", \"MIA\", \"OGG…\n$ distance  &lt;dbl&gt; 2504, 748, 2556, 862, 209, 337, 2218, 2342, 2486, 862, 156, …\n$ time      &lt;dttm&gt; 2022-01-01 13:15:00, 2022-01-01 14:00:00, 2022-01-01 14:45:…"
  },
  {
    "objectID": "pipelines/csv-connect-deploy/index.html#modeling",
    "href": "pipelines/csv-connect-deploy/index.html#modeling",
    "title": "Vetiver and Posit Connect",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3  1988     4 rmse    standard    28.1    10    5.88 Preprocessor1_Model01\n2     8   849    13 rmse    standard    29.6    10    6.44 Preprocessor1_Model04\n3     3  1543     9 rmse    standard    29.6    10    6.11 Preprocessor1_Model02\n4    10  1139    14 rmse    standard    30.0    10    6.44 Preprocessor1_Model05\n5    12   554    18 rmse    standard    30.6    10    6.77 Preprocessor1_Model06\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)"
  },
  {
    "objectID": "pipelines/csv-connect-deploy/index.html#creating-vetiver-model",
    "href": "pipelines/csv-connect-deploy/index.html#creating-vetiver-model",
    "title": "Vetiver and Posit Connect",
    "section": "Creating vetiver model",
    "text": "Creating vetiver model\n\nv &lt;- xgb_last %&gt;%\n  extract_workflow() %&gt;%\n  vetiver_model(\"flights_xgb\")\nv\n\n\n── flights_xgb ─ &lt;bundled_workflow&gt; model for deployment \nA xgboost regression modeling workflow using 7 features"
  },
  {
    "objectID": "pipelines/csv-connect-deploy/index.html#version-model-with-pins-on-posit-connect",
    "href": "pipelines/csv-connect-deploy/index.html#version-model-with-pins-on-posit-connect",
    "title": "Vetiver and Posit Connect",
    "section": "Version model with pins on Posit Connect",
    "text": "Version model with pins on Posit Connect\nWe will version this model on Posit Connect using the pins package.\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are CONNECT_SERVER and CONNECT_API_KEY.\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nCONNECT_SERVER is the URL of the posit connect page. So if your connect server is accessed through https://example.com/connect/#/content/ then you can find CONNECT_SERVER by removing connect/ and everything that follows it, leaving you with https://example.com/.\nCONNECT_API_KEY is created through your Connect server. 1. Click on your name in the upper right upper right. 1. Click API keys. 1. Click New API Key. 1. Give your API a key, click `Create Key.\nOnce you have those two, you can add them to your .Renviron file in the following format:\nCONNECT_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nCONNECT_SERVER=https://example.com/\nNote that you don’t want to put quotes around the values.\nOnce that is all done, we can create a board that connects to Connect, and write our vetiver model to the board.\n\nboard &lt;- board_connect()\nvetiver_pin_write(board, v)\n\nSince we are using vetiver_deploy_rsconnect() which uses the {rsconnect} package, we need to make sure that we have the right authetication and settings. We can do this using rsconnect::connectApiUser().\nrsconnect::connectApiUser(\n  account = \"account.name\",\n  server = \"https://example.com/\", \n  apiKey = \"....\"\n)\nYou will notice that we already have these values, as we got them earlier when we set up the environment variables.\nOnce we are properly set up, we can use vetiver_deploy_rsconnect(), it takes a board and the name of endpoint. Depending on your model, it will take a little while to run as it installs what it needs.\n\nendpoint &lt;- vetiver_deploy_rsconnect(\n  board = board, \n  name = \"emil.hvitfeldt/flights_xgb\"\n)\n\nOnce this finishes running, you might get a pop-up website with your endpoint information. It should also print to the console. You can also find it in Connect itself. What we need is the “CONTENT URL”."
  },
  {
    "objectID": "pipelines/csv-connect-deploy/index.html#make-predictions-from-connect-endpoint",
    "href": "pipelines/csv-connect-deploy/index.html#make-predictions-from-connect-endpoint",
    "title": "Vetiver and Posit Connect",
    "section": "Make predictions from Connect endpoint",
    "text": "Make predictions from Connect endpoint\nNow that we have the content url we can create an endpoint with vetiver_endpoint(), and that endpoint can be used as a way to make predictions. The content url we got earlier should replace $APP_ID below. Note that it is very important that we end the endpoint with /predict.\n\nendpoint &lt;- vetiver_endpoint(\"https://colorado.posit.co/rsc/content/$APP_ID/predict\")\n\nAnd now we are ready to predict! With the endpoint we can pass in some data set to predict with. Authorization is done in a header that uses the CONNECT_API_KEY environment variable we created earlier.\n\npredict(\n  endpoint,\n  flights_training,\n  httr::add_headers(Authorization = paste(\"Key\", Sys.getenv(\"CONNECT_API_KEY\")))\n)\n\n\n\n# A tibble: 2,817 × 1\n    .pred\n    &lt;dbl&gt;\n 1  -1.79\n 2 -13.3 \n 3 -17.3 \n 4  -3.67\n 5  82.8 \n 6  52.2 \n 7  10.7 \n 8   8.04\n 9  58.1 \n10   5.39\n# ℹ 2,807 more rows"
  },
  {
    "objectID": "pipelines/pins-s3-docker/index.html",
    "href": "pipelines/pins-s3-docker/index.html",
    "title": "Pins, Vetiver, S3, and Docker",
    "section": "",
    "text": "Note\n\n\n\nThis page was last generated on 2024-03-13. If you find the code out of date please file an issue."
  },
  {
    "objectID": "pipelines/pins-s3-docker/index.html#loading-packages",
    "href": "pipelines/pins-s3-docker/index.html#loading-packages",
    "title": "Pins, Vetiver, S3, and Docker",
    "section": "Loading packages",
    "text": "Loading packages\nWe are using the tidymodels package to do the modeling, embed for target encoding, pins for versioning, vetiver for version and deployment, and paws.storage for S3 connections.\n\n# install.packages(\"pak\")\n# pak::pak(\"tidymodels\", \"embed\", \"vetiver\", \"pins\", \"paws.storage\")\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(paws.storage)"
  },
  {
    "objectID": "pipelines/pins-s3-docker/index.html#loading-data-from-amazon-s3-with-pins",
    "href": "pipelines/pins-s3-docker/index.html#loading-data-from-amazon-s3-with-pins",
    "title": "Pins, Vetiver, S3, and Docker",
    "section": "Loading data from Amazon S3 with pins",
    "text": "Loading data from Amazon S3 with pins\nWe will fetch data from and version the final model on Posit Connect using the pins package.\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.\n\n\n\n\n\n\nWarning\n\n\n\nDepending on your S3 setup, you will need to use additional variables to connect. Please see https://github.com/paws-r/paws/blob/main/docs/credentials.md and this pins issue for help if the following paragraphs doesn’t work for you.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nYou can find both of these keys in the same location.\n\nOpen the AWS Console\nClick on your username near the top right and select Security Credentials\nClick on Users in the sidebar\nClick on your username\nClick on the Security Credentials tab\nClick Create Access Key\nClick Show User Security Credentials\n\nOnce you have those two, you can add them to your .Renviron file in the following format:\nAWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAWS_ACCESS_KEY_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nNote that you don’t want to put quotes around the values."
  },
  {
    "objectID": "pipelines/pins-s3-docker/index.html#loading-data",
    "href": "pipelines/pins-s3-docker/index.html#loading-data",
    "title": "Pins, Vetiver, S3, and Docker",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the smaller laxflights2022 data set described on the data preparation page. The data set has been uploaded to pins, as described on the data pins page. This is meant to simulate this workflow where we stay inside Connect as much as possible.\n\nboard &lt;- board_s3(\n  \"tidymodels-pipeline-example\",\n  region = \"us-west-1\"\n)\n\nflights &lt;- board |&gt; \n  pin_read(\"laxflights2022_lite\")\n\nglimpse(flights)\n\nRows: 3,757\nColumns: 8\n$ arr_delay &lt;dbl&gt; 4, -15, -12, 38, -9, -17, 5, 12, -40, 6, -7, 28, 25, -9, 180…\n$ dep_delay &lt;dbl&gt; 9, -8, 0, -7, 3, 6, 29, -1, 2, 7, 6, 13, 34, -2, 191, 52, 9,…\n$ carrier   &lt;chr&gt; \"UA\", \"OO\", \"AA\", \"UA\", \"OO\", \"OO\", \"UA\", \"AA\", \"DL\", \"DL\", …\n$ tailnum   &lt;chr&gt; \"N37502\", \"N198SY\", \"N410AN\", \"N77261\", \"N402SY\", \"N509SY\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"KOA\", \"EUG\", \"HNL\", \"DEN\", \"FAT\", \"SFO\", \"MCO\", \"MIA\", \"OGG…\n$ distance  &lt;dbl&gt; 2504, 748, 2556, 862, 209, 337, 2218, 2342, 2486, 862, 156, …\n$ time      &lt;dttm&gt; 2022-01-01 13:15:00, 2022-01-01 14:00:00, 2022-01-01 14:45:…"
  },
  {
    "objectID": "pipelines/pins-s3-docker/index.html#modeling",
    "href": "pipelines/pins-s3-docker/index.html#modeling",
    "title": "Pins, Vetiver, S3, and Docker",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3  1988     4 rmse    standard    28.1    10    5.88 Preprocessor1_Model01\n2     8   849    13 rmse    standard    29.6    10    6.44 Preprocessor1_Model04\n3     3  1543     9 rmse    standard    29.6    10    6.11 Preprocessor1_Model02\n4    10  1139    14 rmse    standard    30.0    10    6.44 Preprocessor1_Model05\n5    12   554    18 rmse    standard    30.6    10    6.77 Preprocessor1_Model06\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)"
  },
  {
    "objectID": "pipelines/pins-s3-docker/index.html#creating-vetiver-model",
    "href": "pipelines/pins-s3-docker/index.html#creating-vetiver-model",
    "title": "Pins, Vetiver, S3, and Docker",
    "section": "Creating vetiver model",
    "text": "Creating vetiver model\n\nv &lt;- xgb_last %&gt;%\n  extract_workflow() %&gt;%\n  vetiver_model(\"flights_xgb\")\nv\n\n\n── flights_xgb ─ &lt;bundled_workflow&gt; model for deployment \nA xgboost regression modeling workflow using 7 features"
  },
  {
    "objectID": "pipelines/pins-s3-docker/index.html#version-model-with-pins-on-amazon-s3",
    "href": "pipelines/pins-s3-docker/index.html#version-model-with-pins-on-amazon-s3",
    "title": "Pins, Vetiver, S3, and Docker",
    "section": "Version model with pins on Amazon S3",
    "text": "Version model with pins on Amazon S3\nWe will version this model on Amazon S3 using the pins package.\nwith the board we specified to read in the data, we can use it to write our model to it. But you are not required to keep them at the same pin.\n\nvetiver_pin_write(board, v)"
  },
  {
    "objectID": "pipelines/pins-s3-docker/index.html#create-docker-artifacts",
    "href": "pipelines/pins-s3-docker/index.html#create-docker-artifacts",
    "title": "Pins, Vetiver, S3, and Docker",
    "section": "Create Docker artifacts",
    "text": "Create Docker artifacts\nTo build a Docker image that can serve your model, you need three artifacts:\n\nthe Dockerfile itself,\na renv.lock to capture your model dependencies, and\nan plumber.R file containing the information to serve a vetiver REST API.\n\nYou can create all the needed files with one function.\n\nvetiver_prepare_docker(\n  board, \n  \"flights_xgb\", \n  docker_args = list(port = 8080)\n)\n\n- The lockfile is already up to date.\n\n\n\n\n\n\n\n\nNote\n\n\n\nKeep an eye on the value of port, we want to make sure we use the same throughout the whole pipeline.\n\n\nFor ease of use, we make sure only to have CRAN versions of packages."
  },
  {
    "objectID": "pipelines/pins-s3-docker/index.html#build-and-run-your-dockerfile",
    "href": "pipelines/pins-s3-docker/index.html#build-and-run-your-dockerfile",
    "title": "Pins, Vetiver, S3, and Docker",
    "section": "Build and run your Dockerfile",
    "text": "Build and run your Dockerfile\nNow that we have everything we need to build a Docker image. We have one more thing to do. Install Docker if you haven’t already, then launch it so we can interact with it from the command line (not from R). Use the following docker build command. Notice that we can give it a “name” using the --tag flag. The . here denotes the path to the build context. Which in this example is the folder we are in.\ndocker build --tag flights .\n\n\n\n\n\n\nTip\n\n\n\nIf you are on an ARM architecture locally and deploying an R model, use --platform linux/amd64 for RSPM’s fast installation of R package binaries.\n\n\nTo run the docker container, we need to pass in the environment variables for the code to connect to the Connect server. We could pass in the system environment variables, but we will be safer if we just pass in what we need. We do this by creating a project-specific .Renviron file. (fs::file_touch(\".Renviron\")) and specifying AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in that file.\nThen we run docker run command. We set 2 flags, --env-file to pass in the environment variables we need, and --publish to specify the port.\ndocker run --env-file .Renviron --publish 8080:8080 flights"
  },
  {
    "objectID": "pipelines/pins-s3-docker/index.html#make-predictions-from-docker-container",
    "href": "pipelines/pins-s3-docker/index.html#make-predictions-from-docker-container",
    "title": "Pins, Vetiver, S3, and Docker",
    "section": "Make predictions from Docker container",
    "text": "Make predictions from Docker container\nNow that the docker container is running we can create an endpoint with vetiver_endpoint(), and that endpoint can be used as a way to make predictions.\n\nendpoint &lt;- vetiver_endpoint(\"http://0.0.0.0:8080/predict\")\n\npredict(endpoint, flights_training)\n\n\n\n# A tibble: 2,817 × 1\n    .pred\n    &lt;dbl&gt;\n 1  -1.79\n 2 -13.3 \n 3 -17.3 \n 4  -3.67\n 5  82.8 \n 6  52.2 \n 7  10.7 \n 8   8.04\n 9  58.1 \n10   5.39\n# ℹ 2,807 more rows"
  },
  {
    "objectID": "pipelines/pins-connect-deploy/index.html",
    "href": "pipelines/pins-connect-deploy/index.html",
    "title": "Vetiver and Posit Connect",
    "section": "",
    "text": "Note\n\n\n\nThis page was last generated on 2024-03-08. If you find the code out of date please file an issue."
  },
  {
    "objectID": "pipelines/pins-connect-deploy/index.html#loading-packages",
    "href": "pipelines/pins-connect-deploy/index.html#loading-packages",
    "title": "Vetiver and Posit Connect",
    "section": "Loading packages",
    "text": "Loading packages\nWe are using the tidymodels package to do the modeling, embed for target encoding, pins for versioning, vetiver for version and deployment, and rsconnect for connecting with Posit Connect.\n\n# install.packages(\"pak\")\n# pak::pak(\"tidymodels\", \"embed\", \"vetiver\", \"pins\", \"rsconnect\")\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(rsconnect)"
  },
  {
    "objectID": "pipelines/pins-connect-deploy/index.html#loading-data-from-posit-connect-with-pins",
    "href": "pipelines/pins-connect-deploy/index.html#loading-data-from-posit-connect-with-pins",
    "title": "Vetiver and Posit Connect",
    "section": "Loading data from Posit Connect with pins",
    "text": "Loading data from Posit Connect with pins\nWe will fetch data from and version the final model on Posit Connect using the pins package.\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are CONNECT_SERVER and CONNECT_API_KEY.\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nCONNECT_SERVER is the URL of the posit connect page. So if your connect server is accessed through https://example.com/connect/#/content/ then you can find CONNECT_SERVER by removing connect/ and everything that follows it, leaving you with https://example.com/.\nCONNECT_API_KEY is created through your Connect server. 1. Click on your name in the upper right upper right. 1. Click API keys. 1. Click New API Key. 1. Give your API a key, click `Create Key.\nOnce you have those two, you can add them to your .Renviron file in the following format:\nCONNECT_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nCONNECT_SERVER=https://example.com/\nNote that you don’t want to put quotes around the values."
  },
  {
    "objectID": "pipelines/pins-connect-deploy/index.html#loading-data",
    "href": "pipelines/pins-connect-deploy/index.html#loading-data",
    "title": "Vetiver and Posit Connect",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the smaller laxflights2022 data set described on the data preparation page. The data set has been uploaded to pins, as described on the data pins page. This is meant to simulate this workflow where we stay inside Connect as much as possible.\n\nboard &lt;- board_connect()\n\nflights &lt;- board |&gt; \n  pin_read(\"emil.hvitfeldt/laxflights2022_lite\")\n\nglimpse(flights)\n\nRows: 3,757\nColumns: 8\n$ arr_delay &lt;dbl&gt; 4, -15, -12, 38, -9, -17, 5, 12, -40, 6, -7, 28, 25, -9, 180…\n$ dep_delay &lt;dbl&gt; 9, -8, 0, -7, 3, 6, 29, -1, 2, 7, 6, 13, 34, -2, 191, 52, 9,…\n$ carrier   &lt;chr&gt; \"UA\", \"OO\", \"AA\", \"UA\", \"OO\", \"OO\", \"UA\", \"AA\", \"DL\", \"DL\", …\n$ tailnum   &lt;chr&gt; \"N37502\", \"N198SY\", \"N410AN\", \"N77261\", \"N402SY\", \"N509SY\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"KOA\", \"EUG\", \"HNL\", \"DEN\", \"FAT\", \"SFO\", \"MCO\", \"MIA\", \"OGG…\n$ distance  &lt;dbl&gt; 2504, 748, 2556, 862, 209, 337, 2218, 2342, 2486, 862, 156, …\n$ time      &lt;dttm&gt; 2022-01-01 13:15:00, 2022-01-01 14:00:00, 2022-01-01 14:45:…"
  },
  {
    "objectID": "pipelines/pins-connect-deploy/index.html#modeling",
    "href": "pipelines/pins-connect-deploy/index.html#modeling",
    "title": "Vetiver and Posit Connect",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3  1988     4 rmse    standard    28.1    10    5.88 Preprocessor1_Model01\n2     8   849    13 rmse    standard    29.6    10    6.44 Preprocessor1_Model04\n3     3  1543     9 rmse    standard    29.6    10    6.11 Preprocessor1_Model02\n4    10  1139    14 rmse    standard    30.0    10    6.44 Preprocessor1_Model05\n5    12   554    18 rmse    standard    30.6    10    6.77 Preprocessor1_Model06\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)"
  },
  {
    "objectID": "pipelines/pins-connect-deploy/index.html#creating-vetiver-model",
    "href": "pipelines/pins-connect-deploy/index.html#creating-vetiver-model",
    "title": "Vetiver and Posit Connect",
    "section": "Creating vetiver model",
    "text": "Creating vetiver model\n\nv &lt;- xgb_last %&gt;%\n  extract_workflow() %&gt;%\n  vetiver_model(\"flights_xgb\")\nv\n\n\n── flights_xgb ─ &lt;bundled_workflow&gt; model for deployment \nA xgboost regression modeling workflow using 7 features"
  },
  {
    "objectID": "pipelines/pins-connect-deploy/index.html#version-model-with-pins-on-posit-connect",
    "href": "pipelines/pins-connect-deploy/index.html#version-model-with-pins-on-posit-connect",
    "title": "Vetiver and Posit Connect",
    "section": "Version model with pins on Posit Connect",
    "text": "Version model with pins on Posit Connect\nOnce that is all done, we can create a board that connects to Connect, and write our vetiver model to the board.\n\nboard &lt;- board_connect()\nvetiver_pin_write(board, v)\n\nSince we are using vetiver_deploy_rsconnect() which uses the {rsconnect} package, we need to make sure that we have the right authetication and settings. We can do this using rsconnect::connectApiUser().\nrsconnect::connectApiUser(\n  account = \"account.name\",\n  server = \"https://example.com/\", \n  apiKey = \"....\"\n)\nYou will notice that we already have these values, as we got them earlier when we set up the environment variables.\nOnce we are properly set up, we can use vetiver_deploy_rsconnect(), it takes a board and the name of endpoint. Depending on your model, it will take a little while to run as it installs what it needs.\n\nendpoint &lt;- vetiver_deploy_rsconnect(\n  board = board, \n  name = \"emil.hvitfeldt/flights_xgb\"\n)\n\nOnce this finishes running, you might get a pop-up website with your endpoint information. It should also print to the console. You can also find it in Connect itself. What we need is the “CONTENT URL”."
  },
  {
    "objectID": "pipelines/pins-connect-deploy/index.html#make-predictions-from-connect-endpoint",
    "href": "pipelines/pins-connect-deploy/index.html#make-predictions-from-connect-endpoint",
    "title": "Vetiver and Posit Connect",
    "section": "Make predictions from Connect endpoint",
    "text": "Make predictions from Connect endpoint\nNow that we have the content url we can create an endpoint with vetiver_endpoint(), and that endpoint can be used as a way to make predictions. The content url we got earlier should replace $APP_ID below. Note that it is very important that we end the endpoint with /predict.\n\nendpoint &lt;- vetiver_endpoint(\"https://colorado.posit.co/rsc/content/$APP_ID/predict\")\n\nAnd now we are ready to predict! With the endpoint we can pass in some data set to predict with. Authorization is done in a header that uses the CONNECT_API_KEY environment variable we created earlier.\n\npredict(\n  endpoint,\n  flights_training,\n  httr::add_headers(Authorization = paste(\"Key\", Sys.getenv(\"CONNECT_API_KEY\")))\n)\n\n\n\n# A tibble: 2,817 × 1\n    .pred\n    &lt;dbl&gt;\n 1  -1.79\n 2 -13.3 \n 3 -17.3 \n 4  -3.67\n 5  82.8 \n 6  52.2 \n 7  10.7 \n 8   8.04\n 9  58.1 \n10   5.39\n# ℹ 2,807 more rows"
  },
  {
    "objectID": "pipelines/csv-azure-docker/index.html",
    "href": "pipelines/csv-azure-docker/index.html",
    "title": "Vetiver, Azure, and Docker",
    "section": "",
    "text": "Note\n\n\n\nThis page was last generated on 2024-03-08. If you find the code out of date please file an issue."
  },
  {
    "objectID": "pipelines/csv-azure-docker/index.html#loading-packages",
    "href": "pipelines/csv-azure-docker/index.html#loading-packages",
    "title": "Vetiver, Azure, and Docker",
    "section": "Loading packages",
    "text": "Loading packages\nWe are using the tidymodels package to do the modeling, embed for target encoding, pins for versioning, vetiver for version and deployment, and AzureStor for connecting with Azure Storage.\n\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(AzureStor)"
  },
  {
    "objectID": "pipelines/csv-azure-docker/index.html#loading-data",
    "href": "pipelines/csv-azure-docker/index.html#loading-data",
    "title": "Vetiver, Azure, and Docker",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the smaller laxflights2022 data set described on the data preparation page.\n\nflights &lt;- readr::read_csv(here::here(\"data/laxflights2022_lite.csv\"))\n\nglimpse(flights)\n\nRows: 3,757\nColumns: 8\n$ arr_delay &lt;dbl&gt; 4, -15, -12, 38, -9, -17, 5, 12, -40, 6, -7, 28, 25, -9, 180…\n$ dep_delay &lt;dbl&gt; 9, -8, 0, -7, 3, 6, 29, -1, 2, 7, 6, 13, 34, -2, 191, 52, 9,…\n$ carrier   &lt;chr&gt; \"UA\", \"OO\", \"AA\", \"UA\", \"OO\", \"OO\", \"UA\", \"AA\", \"DL\", \"DL\", …\n$ tailnum   &lt;chr&gt; \"N37502\", \"N198SY\", \"N410AN\", \"N77261\", \"N402SY\", \"N509SY\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"KOA\", \"EUG\", \"HNL\", \"DEN\", \"FAT\", \"SFO\", \"MCO\", \"MIA\", \"OGG…\n$ distance  &lt;dbl&gt; 2504, 748, 2556, 862, 209, 337, 2218, 2342, 2486, 862, 156, …\n$ time      &lt;dttm&gt; 2022-01-01 13:15:00, 2022-01-01 14:00:00, 2022-01-01 14:45:…"
  },
  {
    "objectID": "pipelines/csv-azure-docker/index.html#modeling",
    "href": "pipelines/csv-azure-docker/index.html#modeling",
    "title": "Vetiver, Azure, and Docker",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3  1988     4 rmse    standard    28.1    10    5.88 Preprocessor1_Model01\n2     8   849    13 rmse    standard    29.6    10    6.44 Preprocessor1_Model04\n3     3  1543     9 rmse    standard    29.6    10    6.11 Preprocessor1_Model02\n4    10  1139    14 rmse    standard    30.0    10    6.44 Preprocessor1_Model05\n5    12   554    18 rmse    standard    30.6    10    6.77 Preprocessor1_Model06\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)"
  },
  {
    "objectID": "pipelines/csv-azure-docker/index.html#creating-vetiver-model",
    "href": "pipelines/csv-azure-docker/index.html#creating-vetiver-model",
    "title": "Vetiver, Azure, and Docker",
    "section": "Creating vetiver model",
    "text": "Creating vetiver model\n\nv &lt;- xgb_last %&gt;%\n  extract_workflow() %&gt;%\n  vetiver_model(\"flights_xgb\")\nv\n\n\n── flights_xgb ─ &lt;bundled_workflow&gt; model for deployment \nA xgboost regression modeling workflow using 7 features"
  },
  {
    "objectID": "pipelines/csv-azure-docker/index.html#version-model-with-pins-on-azure",
    "href": "pipelines/csv-azure-docker/index.html#version-model-with-pins-on-azure",
    "title": "Vetiver, Azure, and Docker",
    "section": "Version model with pins on Azure",
    "text": "Version model with pins on Azure\nWe will version this model on Azure using the pins package.\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are AZURE_CONTAINER_ENDPOINT and AZURE_SAS_KEY.\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nFirst we need to create a Azure container for us to point to. You can find out how to create a storage account and how to create a container from the official documentation. Furthermore, generating a SAS key should be done as well.\nOnce you have those two, you can add them to your .Renviron file in the following format:\nAZURE_CONTAINER_ENDPOINT=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAZURE_SAS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nThe container endpoint will have the following format https://name-of-storage-account.blob.core.windows.net/name-of-container.\nOnce that is all done, we can create a board that connects to Azure, and write our vetiver model to the board. Now that you have set up the environment variables, we can create a pins board.\n\ncontainer &lt;-\n  storage_container(\n    endpoint = Sys.getenv(\"AZURE_CONTAINER_ENDPOINT\"),\n    sas = Sys.getenv(\"AZURE_SAS_KEY\")\n  )\n\nboard &lt;- board_azure(container)\n\nvetiver_pin_write(board, v)"
  },
  {
    "objectID": "pipelines/csv-azure-docker/index.html#create-docker-artifacts",
    "href": "pipelines/csv-azure-docker/index.html#create-docker-artifacts",
    "title": "Vetiver, Azure, and Docker",
    "section": "Create Docker artifacts",
    "text": "Create Docker artifacts\nTo build a Docker image that can serve your model, you need three artifacts:\n\nthe Dockerfile itself,\na renv.lock to capture your model dependencies, and\nan plumber.R file containing the information to serve a vetiver REST API.\n\nYou can create all the needed files with one function.\n\nvetiver_prepare_docker(\n  board, \n  \"flights_xgb\", \n  docker_args = list(port = 8080)\n)\n\n- The lockfile is already up to date.\n\n\n\n\n\n\n\n\nNote\n\n\n\nKeep an eye on the value of port, we want to make sure we use the same throughout the whole pipeline.\n\n\nFor ease of use, we make sure only to have CRAN versions of packages."
  },
  {
    "objectID": "pipelines/csv-azure-docker/index.html#build-and-run-your-dockerfile",
    "href": "pipelines/csv-azure-docker/index.html#build-and-run-your-dockerfile",
    "title": "Vetiver, Azure, and Docker",
    "section": "Build and run your Dockerfile",
    "text": "Build and run your Dockerfile\nNow that we have everything we need to build a Docker image. We have one more thing to do. Install Docker if you haven’t already, then launch it so we can interact with it from the command line (not from R). Use the following docker build command. Notice that we can give it a “name” using the --tag flag. The . here denotes the path to the build context. Which in this example is the folder we are in.\ndocker build --tag flights .\n\n\n\n\n\n\nTip\n\n\n\nIf you are on an ARM architecture locally and deploying an R model, use --platform linux/amd64 for RSPM’s fast installation of R package binaries.\n\n\nTo run the docker container, we need to pass in the environment variables for the code to connect to the Connect server. We could pass in the system environment variables, but we will be safer if we just pass in what we need. We do this by creating a project-specific .Renviron file. (fs::file_touch(\".Renviron\")) and specifying AZURE_CONTAINER_ENDPOINT and AZURE_SAS_KEY in that file.\nThen we run docker run command. We set 2 flags, --env-file to pass in the environment variables we need, and --publish to specify the port.\ndocker run --env-file .Renviron --publish 8080:8080 flights"
  },
  {
    "objectID": "pipelines/csv-azure-docker/index.html#make-predictions-from-docker-container",
    "href": "pipelines/csv-azure-docker/index.html#make-predictions-from-docker-container",
    "title": "Vetiver, Azure, and Docker",
    "section": "Make predictions from Docker container",
    "text": "Make predictions from Docker container\nNow that the docker container is running we can create an endpoint with vetiver_endpoint(), and that endpoint can be used as a way to make predictions.\n\nendpoint &lt;- vetiver_endpoint(\"http://0.0.0.0:8080/predict\")\n\npredict(endpoint, flights_training)\n\n\n\n# A tibble: 2,817 × 1\n    .pred\n    &lt;dbl&gt;\n 1  -1.79\n 2 -13.3 \n 3 -17.3 \n 4  -3.67\n 5  82.8 \n 6  52.2 \n 7  10.7 \n 8   8.04\n 9  58.1 \n10   5.39\n# ℹ 2,807 more rows"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tidymodels pipelines",
    "section": "",
    "text": "The goal of this project is to show how tidymodels can be used with different services, cloud providers and techniques.\nEach pipeline will try to show how tidymodels can be used with another software solution. All the pipelines are expanded versions of the “standard pipeline” which just uses R and tidymodels:\nAll other pipelines can be found in the pipelines list.\nThe modeling problem we are creating solutions for is stated as the following:"
  },
  {
    "objectID": "index.html#data-source",
    "href": "index.html#data-source",
    "title": "tidymodels pipelines",
    "section": "Data source",
    "text": "Data source\nThe data sources used in the pipelines within this project will largely be the same data set. This is done to move focus away from the modeling problem and towards how tidymodels can be used with other software.\nThe data set that is used, is generated using the anyflights package. The standard data set includes all the flights departing from LAX in the year 2022.\n\nlibrary(readr)\n\nlaxflights2022 &lt;- read_csv(\"data/laxflights2022.csv\", show_col_types = FALSE)\n\ndplyr::glimpse(laxflights2022)\n\nRows: 187,868\nColumns: 8\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\n\nWhy this data set?\n\nProposes a realistic enough modeling problem\nMore data can be fetched to showcase larger data problems\nData from more airports can be used together to showcase a “many models” approach\nUSA Government data -&gt; friendly data license"
  },
  {
    "objectID": "pipelines/pins-sagemaker-deploy/index.html",
    "href": "pipelines/pins-sagemaker-deploy/index.html",
    "title": "Pins, Vetiver and Amazon SageMaker",
    "section": "",
    "text": "Note\n\n\n\nThis page was last generated on 2024-03-13. If you find the code out of date please file an issue."
  },
  {
    "objectID": "pipelines/pins-sagemaker-deploy/index.html#loading-packages",
    "href": "pipelines/pins-sagemaker-deploy/index.html#loading-packages",
    "title": "Pins, Vetiver and Amazon SageMaker",
    "section": "Loading packages",
    "text": "Loading packages\nWe are using the tidymodels package to do the modeling, embed for target encoding, pins for versioning, vetiver for version and deployment, and smdocker for deploying to SageMaker.\n\n# install.packages(\"pak\")\n# pak::pak(\"tidymodels\", \"embed\", \"vetiver\", \"pins\", \"smdocker\")\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(smdocker)"
  },
  {
    "objectID": "pipelines/pins-sagemaker-deploy/index.html#loading-data-from-amazon-s3-with-pins",
    "href": "pipelines/pins-sagemaker-deploy/index.html#loading-data-from-amazon-s3-with-pins",
    "title": "Pins, Vetiver and Amazon SageMaker",
    "section": "Loading data from Amazon S3 with pins",
    "text": "Loading data from Amazon S3 with pins\nWe will fetch data from and version the final model on Posit Connect using the pins package.\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.\n\n\n\n\n\n\nWarning\n\n\n\nDepending on your S3 setup, you will need to use additional variables to connect. Please see https://github.com/paws-r/paws/blob/main/docs/credentials.md and this pins issue for help if the following paragraphs doesn’t work for you.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nYou can find both of these keys in the same location.\n\nOpen the AWS Console\nClick on your username near the top right and select Security Credentials\nClick on Users in the sidebar\nClick on your username\nClick on the Security Credentials tab\nClick Create Access Key\nClick Show User Security Credentials\n\nOnce you have those two, you can add them to your .Renviron file in the following format:\nAWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAWS_ACCESS_KEY_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nNote that you don’t want to put quotes around the values."
  },
  {
    "objectID": "pipelines/pins-sagemaker-deploy/index.html#loading-data",
    "href": "pipelines/pins-sagemaker-deploy/index.html#loading-data",
    "title": "Pins, Vetiver and Amazon SageMaker",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the smaller laxflights2022 data set described on the data preparation page. The data set has been uploaded to pins, as described on the data pins page. This is meant to simulate this workflow where we stay inside Connect as much as possible.\n\nboard &lt;- board_s3(\n  \"tidymodels-pipeline-example\",\n  region = \"us-west-1\"\n)\n\nflights &lt;- board |&gt; \n  pin_read(\"laxflights2022_lite\")\n\nglimpse(flights)\n\nRows: 3,757\nColumns: 8\n$ arr_delay &lt;dbl&gt; 4, -15, -12, 38, -9, -17, 5, 12, -40, 6, -7, 28, 25, -9, 180…\n$ dep_delay &lt;dbl&gt; 9, -8, 0, -7, 3, 6, 29, -1, 2, 7, 6, 13, 34, -2, 191, 52, 9,…\n$ carrier   &lt;chr&gt; \"UA\", \"OO\", \"AA\", \"UA\", \"OO\", \"OO\", \"UA\", \"AA\", \"DL\", \"DL\", …\n$ tailnum   &lt;chr&gt; \"N37502\", \"N198SY\", \"N410AN\", \"N77261\", \"N402SY\", \"N509SY\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"KOA\", \"EUG\", \"HNL\", \"DEN\", \"FAT\", \"SFO\", \"MCO\", \"MIA\", \"OGG…\n$ distance  &lt;dbl&gt; 2504, 748, 2556, 862, 209, 337, 2218, 2342, 2486, 862, 156, …\n$ time      &lt;dttm&gt; 2022-01-01 13:15:00, 2022-01-01 14:00:00, 2022-01-01 14:45:…"
  },
  {
    "objectID": "pipelines/pins-sagemaker-deploy/index.html#modeling",
    "href": "pipelines/pins-sagemaker-deploy/index.html#modeling",
    "title": "Pins, Vetiver and Amazon SageMaker",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3  1988     4 rmse    standard    28.1    10    5.88 Preprocessor1_Model01\n2     8   849    13 rmse    standard    29.6    10    6.44 Preprocessor1_Model04\n3     3  1543     9 rmse    standard    29.6    10    6.11 Preprocessor1_Model02\n4    10  1139    14 rmse    standard    30.0    10    6.44 Preprocessor1_Model05\n5    12   554    18 rmse    standard    30.6    10    6.77 Preprocessor1_Model06\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)"
  },
  {
    "objectID": "pipelines/pins-sagemaker-deploy/index.html#creating-vetiver-model",
    "href": "pipelines/pins-sagemaker-deploy/index.html#creating-vetiver-model",
    "title": "Pins, Vetiver and Amazon SageMaker",
    "section": "Creating vetiver model",
    "text": "Creating vetiver model\n\nv &lt;- xgb_last %&gt;%\n  extract_workflow() %&gt;%\n  vetiver_model(\"flights_xgb\")\nv\n\n\n── flights_xgb ─ &lt;bundled_workflow&gt; model for deployment \nA xgboost regression modeling workflow using 7 features"
  },
  {
    "objectID": "pipelines/pins-sagemaker-deploy/index.html#version-model-with-pins-on-amazon-sagemaker",
    "href": "pipelines/pins-sagemaker-deploy/index.html#version-model-with-pins-on-amazon-sagemaker",
    "title": "Pins, Vetiver and Amazon SageMaker",
    "section": "Version model with pins on Amazon SageMaker",
    "text": "Version model with pins on Amazon SageMaker\nWe will version this model on Amazon S3 using the pins package.\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.\n\n\n\n\n\n\nWarning\n\n\n\nDepending on your S3 setup, you will need to use additional variables to connect. Please see https://github.com/paws-r/paws/blob/main/docs/credentials.md and this pins issue for help if the following paragraphs doesn’t work for you.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nYou can find both of these keys in the same location.\n\nOpen the AWS Console\nClick on your username near the top right and select Security Credentials\nClick on Users in the sidebar\nClick on your username\nClick on the Security Credentials tab\nClick Create Access Key\nClick Show User Security Credentials\n\nOnce you have those two, you can add them to your .Renviron file in the following format:\nAWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAWS_ACCESS_KEY_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nNote that you don’t want to put quotes around the values.\nwith the board we specified to read in the data, we can use it to write our model to it. But you are not required to keep them at the same pin.\n\nvetiver_pin_write(board, v)\n\nSince we are using vetiver_deploy_sagemaker() which uses the {smdocker} package, we need to make sure that we have the right authetication and settings.\nIf you are working locally, you will likely need to explicitly set up your execution role to work correctly. Check out Execution role requirements in the smdocker documentation, and especially note that the bucket containing your vetiver model needs to be added as a resource in your IAM role policy.\nOnce we are properly set up, we can use vetiver_deploy_sagemaker(), it takes a board, the name of endpoint and the instance_type Look at the Amazon SageMaker pricing to help you decide what you need. Depending on your model, it will take a little while to run as it installs what it needs.\n\nnew_endpoint &lt;- vetiver_deploy_sagemaker(\n  board = board,\n  name = \"flights_xgb\",\n  instance_type = \"ml.t2.medium\"\n)"
  },
  {
    "objectID": "pipelines/pins-sagemaker-deploy/index.html#make-predictions-from-connect-endpoint",
    "href": "pipelines/pins-sagemaker-deploy/index.html#make-predictions-from-connect-endpoint",
    "title": "Pins, Vetiver and Amazon SageMaker",
    "section": "Make predictions from Connect endpoint",
    "text": "Make predictions from Connect endpoint\nWith the endpoint we can pass in some data set to predict with.\n\npredict(\n  new_endpoint,\n  flights_training\n)\n\n\n\n# A tibble: 2,817 × 1\n    .pred\n    &lt;dbl&gt;\n 1  -1.79\n 2 -13.3 \n 3 -17.3 \n 4  -3.67\n 5  82.8 \n 6  52.2 \n 7  10.7 \n 8   8.04\n 9  58.1 \n10   5.39\n# ℹ 2,807 more rows"
  }
]