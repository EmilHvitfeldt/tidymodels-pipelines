[
  {
    "objectID": "pipelines.html",
    "href": "pipelines.html",
    "title": "",
    "section": "",
    "text": "Standard Modeling Pipeline\n\n\n\n\n\nThis analysis is done using tidymodels R alone\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nVetiver, Posit Connect, and Docker\n\n\n\n\n\nUsing vetiver to version a model on Posit Connect, and predict with it from a Docker container\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nVetiver, S3, and Docker\n\n\n\n\n\nUsing vetiver to version a model in a S3 bucket, and predict with it from a Docker container\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pipelines/standard/index.html",
    "href": "pipelines/standard/index.html",
    "title": "Standard Modeling Pipeline",
    "section": "",
    "text": "We are using the tidymodels package to do the modeling and embed for target encoding.\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(embed)"
  },
  {
    "objectID": "pipelines/standard/index.html#loading-packages",
    "href": "pipelines/standard/index.html#loading-packages",
    "title": "Standard Modeling Pipeline",
    "section": "",
    "text": "We are using the tidymodels package to do the modeling and embed for target encoding.\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(embed)"
  },
  {
    "objectID": "pipelines/standard/index.html#loading-data",
    "href": "pipelines/standard/index.html#loading-data",
    "title": "Standard Modeling Pipeline",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the standard laxflights2022 data set described on the data preparation page.\n\nflights &lt;- readr::read_csv(here::here(\"data/laxflights2022.csv\"))\n\nglimpse(flights)\n\nRows: 187,868\nColumns: 8\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\ndownsampling for speed for now.\n\nflights &lt;- slice_sample(flights, prop = 0.2) %&gt;%\n  arrange(time)"
  },
  {
    "objectID": "pipelines/standard/index.html#modeling",
    "href": "pipelines/standard/index.html#modeling",
    "title": "Standard Modeling Pipeline",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    21  1134    12 rmse    standard    13.5    10   0.704 Preprocessor1_Model10\n2    17  1902    19 rmse    standard    14.3    10   0.909 Preprocessor1_Model08\n3    15   569    15 rmse    standard    15.0    10   0.852 Preprocessor1_Model07\n4    11  1302    27 rmse    standard    15.2    10   1.09  Preprocessor1_Model05\n5    12   989     4 rmse    standard    15.6    10   1.05  Preprocessor1_Model06\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)"
  },
  {
    "objectID": "pipelines/standard/index.html#results",
    "href": "pipelines/standard/index.html#results",
    "title": "Standard Modeling Pipeline",
    "section": "Results",
    "text": "Results\nOnce we have the final model, we can do all kind of analyses. Below we have a truth version prediction plot to showcase how well our model works\n\nxgb_last %&gt;%\n  augment() %&gt;%\n  ggplot(aes(arr_delay, .pred)) +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\") +\n  geom_point(alpha = 0.25) +\n  theme_minimal()\n\n\n\n\nTruth against prediction plot. The model has a hard time with overly long delays."
  },
  {
    "objectID": "data-prep.html",
    "href": "data-prep.html",
    "title": "Data Preperation",
    "section": "",
    "text": "This page goes through how we generate the data set laxflights2022 that is used throughout this project."
  },
  {
    "objectID": "data-prep.html#loading-packages",
    "href": "data-prep.html#loading-packages",
    "title": "Data Preperation",
    "section": "Loading packages",
    "text": "Loading packages\nWe load tidyverse for general data manipulation and anyflights to download the data.\n\nlibrary(tidyverse)\nlibrary(anyflights)"
  },
  {
    "objectID": "data-prep.html#downloading-the-data",
    "href": "data-prep.html#downloading-the-data",
    "title": "Data Preperation",
    "section": "Downloading the data",
    "text": "Downloading the data\nThe data set we are using can be downloaded with a single call to get_flights()\n\nlaxflights2022raw &lt;- get_flights(\"LAX\", year = 2022, months = 1:12)\n\nTo avoid repeated downloads, we save the data set right after we download it. Can be found at data-raw/laxflights2022raw.csv.\n\nwrite_csv(laxflights2022raw, \"data-raw/laxflights2022raw.csv\")\n\n\n\n\n\n\n\nBad internet connection\n\n\n\nIf you are having issues with downloading this data all at once then you can split up the download into smaller chuncks like so:\nlaxflights1 &lt;- anyflights(\"LAX\", 2022, 1:6)\nlaxflights2 &lt;- anyflights(\"LAX\", 2022, 7:12)\n\n\nlaxflights2022raw &lt;- dplyr::bind_rows(\n  laxflights1$flights,\n  laxflights2$flights\n)"
  },
  {
    "objectID": "data-prep.html#cleaning-the-data",
    "href": "data-prep.html#cleaning-the-data",
    "title": "Data Preperation",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nWe will do two things to this data set:\n\nremove redundant variables\nremove variables that won’t be available at prediction time\n\nWhen looking at the data\n\nglimpse(laxflights2022raw)\n\nRows: 191,156\nColumns: 19\n$ year           &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2…\n$ month          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;dbl&gt; 7, 14, 15, 23, 25, 31, 34, 109, 119, 122, 135, 139, 224…\n$ sched_dep_time &lt;dbl&gt; 2359, 2343, 2315, 30, 2259, 2312, 25, 59, 55, 50, 2340,…\n$ dep_delay      &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, …\n$ arr_time       &lt;dbl&gt; 459, 608, 733, 742, 745, 745, 611, 608, 643, 608, 1000,…\n$ sched_arr_time &lt;dbl&gt; 511, 540, 647, 820, 631, 636, 631, 615, 633, 552, 811, …\n$ arr_delay      &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 12…\n$ carrier        &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"…\n$ flight         &lt;dbl&gt; 468, 359, 44, 177, 185, 205, 600, 122, 317, 33, 276, 59…\n$ tailnum        &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509…\n$ origin         &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\",…\n$ dest           &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\",…\n$ air_time       &lt;dbl&gt; 153, 203, 234, 238, 240, 231, 187, 160, 177, 143, 298, …\n$ distance       &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1…\n$ hour           &lt;dbl&gt; 23, 23, 23, 0, 22, 23, 0, 0, 0, 0, 23, 1, 23, 5, 5, 5, …\n$ minute         &lt;dbl&gt; 59, 43, 15, 30, 59, 12, 25, 59, 55, 50, 40, 30, 32, 10,…\n$ time_hour      &lt;dttm&gt; 2022-01-01 23:00:00, 2022-01-01 23:00:00, 2022-01-01 2…\n\n\nthe first thing we notice is that time_hour encodes almost the same information as year, month, day, hour and minute. With minute being the only difference. Let us deal with this by adding the minutes to time_hour and only keeping that variables\n\nlaxflights2022 &lt;- laxflights2022raw |&gt;\n  mutate(time = time_hour + minutes(minute)) |&gt;\n  select(-c(year, month, day, hour, minute, time_hour))\n\nglimpse(laxflights2022)\n\nRows: 191,156\nColumns: 14\n$ dep_time       &lt;dbl&gt; 7, 14, 15, 23, 25, 31, 34, 109, 119, 122, 135, 139, 224…\n$ sched_dep_time &lt;dbl&gt; 2359, 2343, 2315, 30, 2259, 2312, 25, 59, 55, 50, 2340,…\n$ dep_delay      &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, …\n$ arr_time       &lt;dbl&gt; 459, 608, 733, 742, 745, 745, 611, 608, 643, 608, 1000,…\n$ sched_arr_time &lt;dbl&gt; 511, 540, 647, 820, 631, 636, 631, 615, 633, 552, 811, …\n$ arr_delay      &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 12…\n$ carrier        &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"…\n$ flight         &lt;dbl&gt; 468, 359, 44, 177, 185, 205, 600, 122, 317, 33, 276, 59…\n$ tailnum        &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509…\n$ origin         &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\",…\n$ dest           &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\",…\n$ air_time       &lt;dbl&gt; 153, 203, 234, 238, 240, 231, 187, 160, 177, 143, 298, …\n$ distance       &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1…\n$ time           &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 2…\n\n\nNext, we see that the combination of dep_time, sched_dep_time and dep_delay is linearly dependent as dep_time - sched_dep_time = dep_delay, so we can remove sched_dep_time and dep_time without losing any information since we also have sched_dep_time as a function of time.\n\nlaxflights2022 &lt;- laxflights2022 |&gt;\n  select(-c(sched_dep_time, dep_time))\n\nglimpse(laxflights2022)\n\nRows: 191,156\nColumns: 12\n$ dep_delay      &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, …\n$ arr_time       &lt;dbl&gt; 459, 608, 733, 742, 745, 745, 611, 608, 643, 608, 1000,…\n$ sched_arr_time &lt;dbl&gt; 511, 540, 647, 820, 631, 636, 631, 615, 633, 552, 811, …\n$ arr_delay      &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 12…\n$ carrier        &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"…\n$ flight         &lt;dbl&gt; 468, 359, 44, 177, 185, 205, 600, 122, 317, 33, 276, 59…\n$ tailnum        &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509…\n$ origin         &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\",…\n$ dest           &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\",…\n$ air_time       &lt;dbl&gt; 153, 203, 234, 238, 240, 231, 187, 160, 177, 143, 298, …\n$ distance       &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1…\n$ time           &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 2…\n\n\nOn a similar note, since we are trying to predict arr_delay, we can’t have arr_time or air_time in the data set as they allow us to figure out arr_delay from dep_time. With some loss of information, we will also remove sched_arr_time to make the modeling a little easier.\n\nlaxflights2022 &lt;- laxflights2022 |&gt;\n  select(-c(arr_time, air_time, sched_arr_time))\n\nglimpse(laxflights2022)\n\nRows: 191,156\nColumns: 9\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ flight    &lt;dbl&gt; 468, 359, 44, 177, 185, 205, 600, 122, 317, 33, 276, 595, 51…\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\nWe will also be removing the flight variable to have one less high cardionality variable to work with.\n\nlaxflights2022 &lt;- laxflights2022 |&gt;\n  select(-flight)\n\nglimpse(laxflights2022)\n\nRows: 191,156\nColumns: 8\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\nSince we are trying to model arr_delay we exclude all the flights where that information is missing.\n\nlaxflights2022 &lt;- laxflights2022 |&gt;\n  drop_na(arr_delay)\n\nglimpse(laxflights2022)\n\nRows: 187,868\nColumns: 8\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\nLastly, the outcome arr_delay has been moved to the beginning of the data set as it aids in investigating the data.\n\nlaxflights2022 &lt;- laxflights2022 |&gt;\n  relocate(arr_delay)\n\nglimpse(laxflights2022)\n\nRows: 187,868\nColumns: 8\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…"
  },
  {
    "objectID": "data-prep.html#saving-the-data",
    "href": "data-prep.html#saving-the-data",
    "title": "Data Preperation",
    "section": "Saving the data",
    "text": "Saving the data\nThe full data set is now ready and is saved as data/laxflights2022.csv.\n\nwrite_csv(laxflights2022, \"data/laxflights2022.csv\")"
  },
  {
    "objectID": "data-prep.html#data-dictionary",
    "href": "data-prep.html#data-dictionary",
    "title": "Data Preperation",
    "section": "Data dictionary",
    "text": "Data dictionary\n\narr_delay: Arrival delays, in minutes. Negative times represent early arrivals.\ndep_delay: Departure delays, in minutes. Negative times represent early departures.\ncarrier: Two letter carrier abbreviation.\ntailnum: Plane tail number.\norigin: FAA airport code for origin of flight.\ndest: FAA airport code for destination of flght.\ndistance: Distance between airports, in miles.\ntime: Scheduled time of the flight as a POSIXct date, rounded to the nearest minute."
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html",
    "href": "pipelines/csv-connect-docker/index.html",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "",
    "text": "Changes from standard\n\n\n\nAll changes from the standard pipeline is highlighted with a cranberry line to the right."
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#loading-packages",
    "href": "pipelines/csv-connect-docker/index.html#loading-packages",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Loading packages",
    "text": "Loading packages\nWe are using the tidymodels package to do the modeling, embed for target encoding, pins for versioning, and vetiver for version and deployment.\n\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(vetiver)\nlibrary(pins)"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#loading-data",
    "href": "pipelines/csv-connect-docker/index.html#loading-data",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the standard laxflights2022 data set described on the data preparation page.\n\nflights &lt;- readr::read_csv(here::here(\"data/laxflights2022.csv\"))\n\nglimpse(flights)\n\nRows: 187,868\nColumns: 8\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\ndownsampling for speed for now.\n\nflights &lt;- slice_sample(flights, prop = 0.02) %&gt;%\n  arrange(time)"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#modeling",
    "href": "pipelines/csv-connect-docker/index.html#modeling",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3  1988     4 rmse    standard    22.5    10    3.97 Preprocessor1_Model01\n2     3  1543     9 rmse    standard    23.5    10    4.72 Preprocessor1_Model02\n3    11   554    18 rmse    standard    24.9    10    5.21 Preprocessor1_Model06\n4     8   849    13 rmse    standard    25.0    10    4.83 Preprocessor1_Model04\n5    10  1139    14 rmse    standard    25.1    10    4.81 Preprocessor1_Model05\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#creating-vetiver-model",
    "href": "pipelines/csv-connect-docker/index.html#creating-vetiver-model",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Creating vetiver model",
    "text": "Creating vetiver model\n\nv &lt;- xgb_last %&gt;%\n  extract_workflow() %&gt;%\n  vetiver_model(\"flights_xgb\")\nv\n\n\n── flights_xgb ─ &lt;bundled_workflow&gt; model for deployment \nA xgboost regression modeling workflow using 7 features"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#version-model-with-pins-on-posit-connect",
    "href": "pipelines/csv-connect-docker/index.html#version-model-with-pins-on-posit-connect",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Version model with pins on Posit Connect",
    "text": "Version model with pins on Posit Connect\nWe will version this model on Posit Connect using the pins package.\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are CONNECT_SERVER and CONNECT_API_KEY.\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nCONNECT_SERVER is the URL of the posit connect page. So if your connect server is accessed through https://example.com/connect/#/content/ then you can find CONNECT_SERVER by removing connect/ and everything that follows it, leaving you with https://example.com/.\nCONNECT_API_KEY is created through your Connect server. 1. Click on your name in the upper right upper right. 1. Click API keys. 1. Click New API Key. 1. Give your API a key, click `Create Key.\nOnce you have those two, you can add them to your .Renviron file in the following format:\nCONNECT_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nCONNECT_SERVER=https://example.com/\nNote that you don’t want to put quotes around the values. Once that is all done, we can create a board that connects to Connect, and write our vetiver model to the board.\n\nboard &lt;- board_connect()\nvetiver_pin_write(board, v)"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#create-docker-artifacts",
    "href": "pipelines/csv-connect-docker/index.html#create-docker-artifacts",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Create Docker artifacts",
    "text": "Create Docker artifacts\nTo build a Docker image that can serve your model, you need three artifacts:\n\nthe Dockerfile itself,\na renv.lock to capture your model dependencies, and\nan plumber.R file containing the information to serve a vetiver REST API.\n\nYou can create all the needed files with one function.\n\nvetiver_prepare_docker(\n  board, \n  \"emil.hvitfeldt/flights_xgb\", \n  docker_args = list(port = 8080)\n)\n\n- The lockfile is already up to date.\n\n\n\n\n\n\n\n\nNote\n\n\n\nKeep an eye on the value of port, we want to make sure we use the same throughout the whole pipeline.\n\n\nFor ease of use, we make sure only to have CRAN versions of packages."
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#build-and-run-your-dockerfile",
    "href": "pipelines/csv-connect-docker/index.html#build-and-run-your-dockerfile",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Build and run your Dockerfile",
    "text": "Build and run your Dockerfile\nNow that we have everything we need to build a Docker image. We have one more thing to do. Install Docker if you haven’t already, then launch it so we can interact with it from the command line (not from R). Use the following docker build command. Notice that we can give it a “name” using the --tag flag. The . here denotes the path to the build context. Which in this example is the folder we are in.\ndocker build --tag flights .\n\n\n\n\n\n\nTip\n\n\n\nIf you are on an ARM architecture locally and deploying an R model, use --platform linux/amd64 for RSPM’s fast installation of R package binaries.\n\n\nTo run the docker container, we need to pass in the environment variables for the code to connect to the Connect server. We could pass in the system environment variables, but we will be safer if we just pass in what we need. We do this by creating a project-specific .Renviron file. (fs::file_touch(\".Renviron\")) and specifying CONNECT_SERVER and CONNECT_API_KEY in that file.\nThen we run docker run command. We set 2 flags, --env-file to pass in the environment variables we need, and --publish to specify the port.\ndocker run --env-file .Renviron --publish 8080:8080 flights"
  },
  {
    "objectID": "pipelines/csv-connect-docker/index.html#make-predictions-from-docker-container",
    "href": "pipelines/csv-connect-docker/index.html#make-predictions-from-docker-container",
    "title": "Vetiver, Posit Connect, and Docker",
    "section": "Make predictions from Docker container",
    "text": "Make predictions from Docker container\nNow that the docker container is running we can create an endpoint with vetiver_endpoint(), and that endpoint can be used as a way to make predictions.\n\nendpoint &lt;- vetiver_endpoint(\"http://0.0.0.0:8080/predict\")\n\npredict(endpoint, flights_training)\n\n\n\n# A tibble: 2,817 × 1\n    .pred\n    &lt;dbl&gt;\n 1  -1.79\n 2 -13.3 \n 3 -17.3 \n 4  -3.67\n 5  82.8 \n 6  52.2 \n 7  10.7 \n 8   8.04\n 9  58.1 \n10   5.39\n# ℹ 2,807 more rows"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tidymodels pipelines",
    "section": "",
    "text": "The goal of this project is to show how tidymodels can be used with different services, cloud providers and techniques.\nEach pipeline will try to show how tidymodels can be used with another software solution. All the pipelines are expanded versions of the “standard pipeline” which just uses R and tidymodels:\nAll other pipelines can be found in the pipelines list.\nThe modeling problem we are creating solutions for is stated as the following:"
  },
  {
    "objectID": "index.html#data-source",
    "href": "index.html#data-source",
    "title": "tidymodels pipelines",
    "section": "Data source",
    "text": "Data source\nThe data sources used in the pipelines within this project will largely be the same data set. This is done to move focus away from the modeling problem and towards how tidymodels can be used with other software.\nThe data set that is used, is generated using the anyflights package. The standard data set includes all the flights departing from LAX in the year 2022.\n\nlibrary(readr)\n\nlaxflights2022 &lt;- read_csv(\"data/laxflights2022.csv\", show_col_types = FALSE)\n\ndplyr::glimpse(laxflights2022)\n\nRows: 187,868\nColumns: 8\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\n\nWhy this data set?\n\nProposes a realistic enough modeling problem\nMore data can be fetched to showcase larger data problems\nData from more airports can be used together to showcase a “many models” approach\nUSA Government data -&gt; friendly data license"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html",
    "href": "pipelines/csv-s3-docker/index.html",
    "title": "Vetiver, S3, and Docker",
    "section": "",
    "text": "Changes from standard\n\n\n\nAll changes from the standard pipeline is highlighted with a cranberry line to the right."
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#loading-packages",
    "href": "pipelines/csv-s3-docker/index.html#loading-packages",
    "title": "Vetiver, S3, and Docker",
    "section": "Loading packages",
    "text": "Loading packages\nWe are using the tidymodels package to do the modeling, embed for target encoding, pins for versioning, vetiver for version and deployment, and paws.storage for S3 connections.\n\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(paws.storage)"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#loading-data",
    "href": "pipelines/csv-s3-docker/index.html#loading-data",
    "title": "Vetiver, S3, and Docker",
    "section": "Loading Data",
    "text": "Loading Data\nWe are using the standard laxflights2022 data set described on the data preparation page.\n\nflights &lt;- readr::read_csv(here::here(\"data/laxflights2022.csv\"))\n\nglimpse(flights)\n\nRows: 187,868\nColumns: 8\n$ arr_delay &lt;dbl&gt; -12, 28, 46, -38, 74, 69, -20, -7, 10, 16, 109, -12, 122, -1…\n$ dep_delay &lt;dbl&gt; 8, 31, 60, -7, 86, 79, 9, 10, 24, 32, 115, 9, 172, -2, 16, -…\n$ carrier   &lt;chr&gt; \"UA\", \"AA\", \"NK\", \"AA\", \"NK\", \"NK\", \"UA\", \"NK\", \"DL\", \"NK\", …\n$ tailnum   &lt;chr&gt; \"N57864\", \"N919NN\", \"N949NK\", \"N812AA\", \"N903NK\", \"N509NK\", …\n$ origin    &lt;chr&gt; \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX\", \"LAX…\n$ dest      &lt;chr&gt; \"IAH\", \"BNA\", \"CLE\", \"PHL\", \"PIT\", \"DTW\", \"ORD\", \"IAH\", \"MSP…\n$ distance  &lt;dbl&gt; 1379, 1797, 2052, 2402, 2136, 1979, 1744, 1379, 1535, 1235, …\n$ time      &lt;dttm&gt; 2022-01-01 23:59:00, 2022-01-01 23:43:00, 2022-01-01 23:15:…\n\n\ndownsampling for speed for now.\n\nflights &lt;- slice_sample(flights, prop = 0.02) %&gt;%\n  arrange(time)"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#modeling",
    "href": "pipelines/csv-s3-docker/index.html#modeling",
    "title": "Vetiver, S3, and Docker",
    "section": "Modeling",
    "text": "Modeling\nAs a reminder, the modeling task we are trying to accomplish is the following:\n\nGiven all the information we have, from the moment the plane leaves for departure. Can we predict the arrival delay arr_delay.\n\nOur outcome is arr_delay and the remaining variables are predictors. We will be fitting a xgboost model as a regression model.\n\nSplitting Data\nSince the data set is already in chronological order, we can create a time split of the data using initial_time_split(), this will put the first 75% of the data into the training data set, and the remaining 25% into the testing data set.\n\nset.seed(1234)\n\nflights_split &lt;- initial_time_split(flights, prop = 3/4)\nflights_training &lt;- training(flights_split)\n\nSince we are doing hyperparameter tuning, we will also be creating a cross-validation split\n\nflights_folds &lt;- vfold_cv(flights_training)\n\n\n\nFeature Engineering\nWe need to do a couple of things to make this data set work for our model. The datetime variable time needs to be transformed, as does the categorical variables carrier, tailnum, origin and dest.\nFrom the time variable, the month and day of the week are extracted as categorical variables, then the day of year and time of day are extracted as numerics. The origin and dest variables will be turned into dummy variables, and carrier, tailnum, time_month, and time_dow will be converted to numerics with likelihood encoding.\n\nflights_rec &lt;- recipe(arr_delay ~ ., data = flights_training) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_other(origin, dest, threshold = 0.025) %&gt;%\n  step_dummy(origin, dest) %&gt;%\n  step_date(time, \n            features = c(\"month\", \"dow\", \"doy\"), \n            label = TRUE, \n            keep_original_cols = TRUE) %&gt;%\n  step_time(time, features = \"decimal_day\", keep_original_cols = FALSE) %&gt;%\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(arr_delay)) %&gt;%\n  step_zv(all_predictors())\n\n\n\nSpecifying Models\nWe will be fitting a boosted tree model in the form of a xgboost model.\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    learn_rate = 0.01\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_wf &lt;- workflow(flights_rec, xgb_spec)\n\n\n\nHyperparameter Tuning\n\ndoParallel::registerDoParallel()\n\nxgb_rs &lt;- tune_grid(\n  xgb_wf,\n  resamples = flights_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nWe can visualize the performance of the different hyperparameter selections\n\nautoplot(xgb_rs)\n\n\n\n\nand look at the top result\n\nshow_best(xgb_rs, metric = \"rmse\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     2  1988     4 rmse    standard    18.3    10    3.28 Preprocessor1_Model01\n2     3  1543     9 rmse    standard    19.5    10    3.48 Preprocessor1_Model02\n3     8  1139    14 rmse    standard    19.9    10    3.66 Preprocessor1_Model05\n4     6   849    13 rmse    standard    20.2    10    3.58 Preprocessor1_Model04\n5     9   554    18 rmse    standard    21.0    10    3.89 Preprocessor1_Model06\n\n\n\n\nFitting Final Model\nOnce we are satisfied with the modeling that has been done, we can fit our final model. We use finalize_workflow() to use the best hyperparameters, and last_fit() to fit the model to the training data set and evaluate it on the testing data set.\n\nxgb_last &lt;- xgb_wf %&gt;%\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) %&gt;%\n  last_fit(flights_split)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#creating-vetiver-model",
    "href": "pipelines/csv-s3-docker/index.html#creating-vetiver-model",
    "title": "Vetiver, S3, and Docker",
    "section": "Creating vetiver model",
    "text": "Creating vetiver model\n\nv &lt;- xgb_last %&gt;%\n  extract_workflow() %&gt;%\n  vetiver_model(\"flights_xgb\")\nv\n\n\n── flights_xgb ─ &lt;bundled_workflow&gt; model for deployment \nA xgboost regression modeling workflow using 7 features"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#version-model-with-pins-on-amazon-s3",
    "href": "pipelines/csv-s3-docker/index.html#version-model-with-pins-on-amazon-s3",
    "title": "Vetiver, S3, and Docker",
    "section": "Version model with pins on Amazon S3",
    "text": "Version model with pins on Amazon S3\nWe will version this model on Amazon S3 using the pins package.\nFor the smoothest experience, we recommend that you authenticate using environment variables. The two variables you will need are AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.\n\n\n\n\n\n\nWarning\n\n\n\nDepending on your S3 setup, you will need to use additional variables to connect. Please see https://github.com/paws-r/paws/blob/main/docs/credentials.md and this pins issue for help if the following paragraphs doesn’t work for you.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe function usethis::edit_r_environ() can be very handy to open .Renviron file to specify your environment variables.\n\n\nYou can find both of these keys in the same location.\n\nOpen the AWS Console\nClick on your username near the top right and select Security Credentials\nClick on Users in the sidebar\nClick on your username\nClick on the Security Credentials tab\nClick Create Access Key\nClick Show User Security Credentials\n\nOnce you have those two, you can add them to your .Renviron file in the following format:\nAWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAWS_ACCESS_KEY_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxx\nNote that you don’t want to put quotes around the values. Once that is all done, we can create a board that connects to Amazon S3, and write our vetiver model to the board. Now that you have set up the environment variables, we can create a pins board. When using S3 you need to specify a bucket and its region. This cannot be done with Pins and has to be done beforehand.\n\nboard &lt;- board_s3(\n  \"tidymodels-pipeline-example\",\n  region = \"us-west-1\"\n)\nvetiver_pin_write(board, v)"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#create-docker-artifacts",
    "href": "pipelines/csv-s3-docker/index.html#create-docker-artifacts",
    "title": "Vetiver, S3, and Docker",
    "section": "Create Docker artifacts",
    "text": "Create Docker artifacts\nTo build a Docker image that can serve your model, you need three artifacts:\n\nthe Dockerfile itself,\na renv.lock to capture your model dependencies, and\nan plumber.R file containing the information to serve a vetiver REST API.\n\nYou can create all the needed files with one function.\n\nvetiver_prepare_docker(\n  board, \n  \"flights_xgb\", \n  docker_args = list(port = 8080)\n)\n\n- The lockfile is already up to date.\n\n\n\n\n\n\n\n\nNote\n\n\n\nKeep an eye on the value of port, we want to make sure we use the same throughout the whole pipeline.\n\n\nFor ease of use, we make sure only to have CRAN versions of packages."
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#build-and-run-your-dockerfile",
    "href": "pipelines/csv-s3-docker/index.html#build-and-run-your-dockerfile",
    "title": "Vetiver, S3, and Docker",
    "section": "Build and run your Dockerfile",
    "text": "Build and run your Dockerfile\nNow that we have everything we need to build a Docker image. We have one more thing to do. Install Docker if you haven’t already, then launch it so we can interact with it from the command line (not from R). Use the following docker build command. Notice that we can give it a “name” using the --tag flag. The . here denotes the path to the build context. Which in this example is the folder we are in.\ndocker build --tag flights .\n\n\n\n\n\n\nTip\n\n\n\nIf you are on an ARM architecture locally and deploying an R model, use --platform linux/amd64 for RSPM’s fast installation of R package binaries.\n\n\nTo run the docker container, we need to pass in the environment variables for the code to connect to the Connect server. We could pass in the system environment variables, but we will be safer if we just pass in what we need. We do this by creating a project-specific .Renviron file. (fs::file_touch(\".Renviron\")) and specifying AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in that file.\nThen we run docker run command. We set 2 flags, --env-file to pass in the environment variables we need, and --publish to specify the port.\ndocker run --env-file .Renviron --publish 8080:8080 flights"
  },
  {
    "objectID": "pipelines/csv-s3-docker/index.html#make-predictions-from-docker-container",
    "href": "pipelines/csv-s3-docker/index.html#make-predictions-from-docker-container",
    "title": "Vetiver, S3, and Docker",
    "section": "Make predictions from Docker container",
    "text": "Make predictions from Docker container\nNow that the docker container is running we can create an endpoint with vetiver_endpoint(), and that endpoint can be used as a way to make predictions.\n\nendpoint &lt;- vetiver_endpoint(\"http://0.0.0.0:8080/predict\")\n\npredict(endpoint, flights_training)\n\n\n\n# A tibble: 2,817 × 1\n    .pred\n    &lt;dbl&gt;\n 1  -1.79\n 2 -13.3 \n 3 -17.3 \n 4  -3.67\n 5  82.8 \n 6  52.2 \n 7  10.7 \n 8   8.04\n 9  58.1 \n10   5.39\n# ℹ 2,807 more rows"
  }
]